{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Deep Learning - Convolutional Network Ensemble (Average)</b></center><br>\n",
    "\n",
    "The code below implements multiple convolutional networks and utilises them in an ensemble to improve prediction. The ensemble method used here is simply averaging with a slight adjustment based on the accuracy of the architecture. The predicted weights from each architecture are multiplied so stronger performing models' prediction get a higher weight then they are added together and the class with the highest prediction is selected.<br> <br>\n",
    "The dataset is very small, it contains 17 categories but only 1020 images in total. To adjust for this, data augmentation is implemented also the data is only split into 'training' and 'validation' sets, 'test' sets are not being utilised.<br> <br>\n",
    "Data checkpointing is also used that caches the best model with the smallest validation loss, this allows incremental training. Custom data checkpointing was used to compensate for a shortcoming of Keras' built-in data checkpointer as that system loses the information about the validation loss when loading the data back in. This custom checkpointer saves the validation loss into a separate file.<br>\n",
    "<br>\n",
    "As can be seen on the results, the ensemble method resulted in a better performance than any of the architectures did by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "Ho5hTcQaHgNq",
    "outputId": "5bc145da-8017-4822-f71b-f4ce7f4c0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Lt_ZAF3vIm0T",
    "outputId": "0d3e629d-c1fc-4bb1-cb9d-983f34334667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 128, 128, 3) (1020,)\n",
      "(340, 128, 128, 3) (340,)\n"
     ]
    }
   ],
   "source": [
    "# Handle dataset\n",
    "FILE_PATH = os.getcwd()\n",
    "CACHE_PATH = FILE_PATH+\"/cached/deeplearning/\"\n",
    "data_file = FILE_PATH+\"/data/DeepLearning/conv_net_data.zip\"\n",
    "\n",
    "data = zipfile.ZipFile(data_file)\n",
    "data_file = data.open('data1.h5')\n",
    "\n",
    "def loadDataH5():\n",
    "    with h5py.File(data_file,'r') as hf:\n",
    "      trainX = np.array(hf.get('trainX'))\n",
    "      trainY = np.array(hf.get('trainY'))\n",
    "      valX = np.array(hf.get('valX'))\n",
    "      valY = np.array(hf.get('valY'))\n",
    "      print (trainX.shape,trainY.shape)\n",
    "      print (valX.shape,valY.shape)\n",
    "      return trainX, trainY, valX, valY \n",
    " \n",
    "trainX, trainY, testX, testY = loadDataH5() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vKSuUaOSJl_"
   },
   "outputs": [],
   "source": [
    "# const variables about the dataset\n",
    "IMG_DEPTH   = 3\n",
    "IMG_WIDTH   = 128\n",
    "IMG_HEIGHT  = 128\n",
    "NUM_CLASSES = 17\n",
    "\n",
    "\n",
    "NUM_EPOCHS  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHJLcMHljA6r"
   },
   "outputs": [],
   "source": [
    "# All Architectures used\n",
    "\n",
    "def create_case_0_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    # add conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_1_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_2_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_3_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 4\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_alex_net_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(96, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 5\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_vgg_16_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "    model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vmrahc9WQQ4H"
   },
   "outputs": [],
   "source": [
    "# Base class for all architectures to derive from. Handles checkpointing and data augmentation\n",
    "class CNN_base_augmented_checkpointed:\n",
    "  def __init__(self, try_to_load_weights=True, ext_name=\"\"):\n",
    "    # Initialise Variables\n",
    "    self.BATCH_SIZE = 16\n",
    "  \n",
    "    # Initialise model\n",
    "    self.model = self.create_model(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH, NUM_CLASSES)\n",
    "    self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Saved data file names\n",
    "    self.weights_file_name = \"{}/{}{}.hdf5\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "    self.val_loss_file_name = \"{}/{}{}_val_loss.txt\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "\n",
    "    # The current best val_loss\n",
    "    self.best_val_loss = float(\"inf\")\n",
    "    if try_to_load_weights:\n",
    "      self.load_checkpointed_weights()\n",
    "    \n",
    "\n",
    "  def load_best_val_loss_value(self):\n",
    "    \"\"\"\n",
    "    This function is used to load the validation loss of the best model.\n",
    "    It is stored in a txt file when the best model's weights were cached\n",
    "    \"\"\"\n",
    "    if os.path.isfile(self.val_loss_file_name):\n",
    "      loss_file = open(self.val_loss_file_name, \"r\")\n",
    "      self.best_val_loss = float(loss_file.read())\n",
    "      loss_file.close()\n",
    "      print(\"{} - Loaded best val_loss value: {:.4f} from: {}\".format(self.get_name(), self.best_val_loss, self.val_loss_file_name))\n",
    "\n",
    "\n",
    "  def save_weights_callback(self, logs):\n",
    "    \"\"\"\n",
    "    This is a custom callback function to be used to cache the weights of the network when val_loss decreases\n",
    "    \"\"\"\n",
    "    val_loss = logs.get('val_loss')\n",
    "    if val_loss < self.best_val_loss:\n",
    "      print(\"\\n{} - Caching Checkpoint - val_loss has improved from: {:.4f} to: {:.4f}\".format(self.get_name(), self.best_val_loss, val_loss))\n",
    "      self.best_val_loss = val_loss\n",
    "      \n",
    "      self.model.save_weights(self.weights_file_name)\n",
    "\n",
    "      loss_file = open(self.val_loss_file_name,\"w\")\n",
    "      loss_file.write(str(val_loss))\n",
    "      loss_file.close()\n",
    "    else:\n",
    "      print(\"\\n{} Val_loss did not improve. Best is: {:.4f}. Not Caching\".format(self.get_name(), self.best_val_loss))\n",
    "\n",
    "  def train(self, tr_x, tr_y, val_x, val_y, num_epochs=NUM_EPOCHS):\n",
    "    print(\"{} - Training for {} epochs\".format(self.get_name(), num_epochs))\n",
    "\n",
    "    # Create the image generator for Data Augmentation\n",
    "    datagen_batch = len(tr_x) / self.BATCH_SIZE\n",
    "    datagen = self.create_image_generator()\n",
    "\n",
    "    # Create the model checkpointer\n",
    "    custom_checkpointing_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs:self.save_weights_callback(logs))\n",
    "\n",
    "    # Train\n",
    "    self.model.fit(datagen.flow(tr_x, tr_y, batch_size=self.BATCH_SIZE), steps_per_epoch=datagen_batch, epochs=num_epochs, validation_data=(val_x, val_y), callbacks=[custom_checkpointing_callback])\n",
    "\n",
    "  def load_checkpointed_weights(self):\n",
    "    # If possible load the previous training results. Weights and assosiated val_loss\n",
    "    if os.path.isfile(self.weights_file_name):\n",
    "      print(\"{} - Loading previous training data\".format(self.get_name()))\n",
    "      self.model.load_weights(self.weights_file_name)\n",
    "      print(\"{} - Loaded previous model weights from: {}\".format(self.get_name(), self.weights_file_name))\n",
    "      self.load_best_val_loss_value()\n",
    "\n",
    "  def get_accuracy_multiplier(self, tr_x, tr_y):\n",
    "    # Get Prediction\n",
    "    pred_y = self.predict(tr_x)\n",
    "    # Calculate Accuracy\n",
    "    pred_y = tf.math.argmax(pred_y, 1)\n",
    "    match_array = tf.equal(pred_y, tr_y)\n",
    "    accuracy = tf.reduce_sum(tf.cast(match_array, tf.float32)) / len(tr_y)\n",
    "    return 1.0 + accuracy\n",
    "\n",
    "  def predict(self, val_x):\n",
    "    return self.model.predict(val_x)\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def create_image_generator(self):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_name(self):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xatn1Sk4Skob"
   },
   "outputs": [],
   "source": [
    "# Similar to Alex-net\n",
    "class alex_net_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Alex_Net\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=40,\n",
    "       width_shift_range=0.2,\n",
    "       height_shift_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_alex_net_architecture(width, height, depth, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEkI4BBIVa15"
   },
   "outputs": [],
   "source": [
    "# Similar to VGG 16\n",
    "class vgg_16_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"VGG_16\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=30,\n",
    "       width_shift_range=0.3,\n",
    "       height_shift_range=0.3,\n",
    "       horizontal_flip=False,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_vgg_16_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 0\n",
    "class case_0_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_0\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=20,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_0_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5taidIni1IY"
   },
   "outputs": [],
   "source": [
    "# Case 1 \n",
    "class case_1_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_1\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=50,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_1_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7teNYHJk7tA"
   },
   "outputs": [],
   "source": [
    "# Case 2\n",
    "class case_2_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_2\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=75,\n",
    "       width_shift_range=0.4,\n",
    "       height_shift_range=0.4,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.1)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_2_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekEkWY3xlwoV"
   },
   "outputs": [],
   "source": [
    "# Case 3\n",
    "class case_3_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_3\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=10,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.3)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_3_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGKmAV-Fy6K2"
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate loss and accuracy\n",
    "def get_prediction_result(predicted_results, y_labels):\n",
    "  # Calculate Loss\n",
    "  y_labels_hot = np_utils.to_categorical(y_labels, NUM_CLASSES)\n",
    "  predicted_results_clipped = tf.clip_by_value(predicted_results, 1e-10, 1.0)\n",
    "  loss = (1.0/predicted_results_clipped.shape[0]) * tf.math.reduce_sum(-tf.math.multiply(tf.math.log(predicted_results_clipped),y_labels_hot))\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  predicted_y = tf.math.argmax(predicted_results, 1)\n",
    "  match_array = tf.equal(predicted_y, y_labels)\n",
    "  accuracy = tf.reduce_sum(tf.cast(match_array, tf.float32)) / len(y_labels)\n",
    "\n",
    "  return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "UeWZmpExi0WP",
    "outputId": "a6933397-41ad-48cf-a329-5621fc5b7687",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CNN architectures\n",
      "Alex_Net - Loading previous training data\n",
      "Alex_Net - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_Net.hdf5\n",
      "Alex_Net - Loaded best val_loss value: 1.3948 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_Net_val_loss.txt\n",
      "VGG_16 - Loading previous training data\n",
      "VGG_16 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16.hdf5\n",
      "VGG_16 - Loaded best val_loss value: 1.4996 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16_val_loss.txt\n",
      "Case_0 - Loading previous training data\n",
      "Case_0 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0.hdf5\n",
      "Case_0 - Loaded best val_loss value: 1.0161 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0_val_loss.txt\n",
      "Case_1 - Loading previous training data\n",
      "Case_1 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1.hdf5\n",
      "Case_1 - Loaded best val_loss value: 1.0186 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1_val_loss.txt\n",
      "Case_2 - Loading previous training data\n",
      "Case_2 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2.hdf5\n",
      "Case_2 - Loaded best val_loss value: 1.2793 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2_val_loss.txt\n",
      "Case_3 - Loading previous training data\n",
      "Case_3 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3.hdf5\n",
      "Case_3 - Loaded best val_loss value: 1.2905 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3_val_loss.txt\n",
      "Training CNN architectures for 15 epochs\n",
      "Alex_Net - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.4154 - accuracy: 0.4861\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3948 to: 1.3836\n",
      "64/63 [==============================] - 252s 4s/step - loss: 1.4162 - accuracy: 0.4873 - val_loss: 1.3836 - val_accuracy: 0.5206\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3852 - accuracy: 0.5120\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3836 to: 1.3750\n",
      "64/63 [==============================] - 242s 4s/step - loss: 1.3890 - accuracy: 0.5098 - val_loss: 1.3750 - val_accuracy: 0.5412\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.4135 - accuracy: 0.4910\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3750 to: 1.3669\n",
      "64/63 [==============================] - 238s 4s/step - loss: 1.4136 - accuracy: 0.4922 - val_loss: 1.3669 - val_accuracy: 0.5412\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.4228 - accuracy: 0.4980\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3669. Not Caching\n",
      "64/63 [==============================] - 235s 4s/step - loss: 1.4242 - accuracy: 0.4961 - val_loss: 1.4120 - val_accuracy: 0.5471\n",
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3715 - accuracy: 0.5080\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3669. Not Caching\n",
      "64/63 [==============================] - 230s 4s/step - loss: 1.3788 - accuracy: 0.5029 - val_loss: 1.3728 - val_accuracy: 0.5647\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3865 - accuracy: 0.5159\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3669. Not Caching\n",
      "64/63 [==============================] - 217s 3s/step - loss: 1.3827 - accuracy: 0.5167 - val_loss: 1.3818 - val_accuracy: 0.5471\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3610 - accuracy: 0.5139\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3669 to: 1.3628\n",
      "64/63 [==============================] - 233s 4s/step - loss: 1.3658 - accuracy: 0.5127 - val_loss: 1.3628 - val_accuracy: 0.5559\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3523 - accuracy: 0.5309\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3628. Not Caching\n",
      "64/63 [==============================] - 231s 4s/step - loss: 1.3488 - accuracy: 0.5353 - val_loss: 1.3643 - val_accuracy: 0.5471\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3489 - accuracy: 0.5179\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3628 to: 1.3511\n",
      "64/63 [==============================] - 229s 4s/step - loss: 1.3465 - accuracy: 0.5196 - val_loss: 1.3511 - val_accuracy: 0.5588\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3837 - accuracy: 0.5110\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3511 to: 1.3390\n",
      "64/63 [==============================] - 229s 4s/step - loss: 1.3871 - accuracy: 0.5078 - val_loss: 1.3390 - val_accuracy: 0.5441\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3716 - accuracy: 0.5070\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3390. Not Caching\n",
      "64/63 [==============================] - 235s 4s/step - loss: 1.3717 - accuracy: 0.5049 - val_loss: 1.3529 - val_accuracy: 0.5500\n",
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.4082 - accuracy: 0.5060\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3390. Not Caching\n",
      "64/63 [==============================] - 224s 4s/step - loss: 1.4141 - accuracy: 0.5020 - val_loss: 1.3396 - val_accuracy: 0.5735\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3401 - accuracy: 0.5060\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3390. Not Caching\n",
      "64/63 [==============================] - 234s 4s/step - loss: 1.3397 - accuracy: 0.5059 - val_loss: 1.3592 - val_accuracy: 0.5412\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3349 - accuracy: 0.5279\n",
      "Alex_Net Val_loss did not improve. Best is: 1.3390. Not Caching\n",
      "64/63 [==============================] - 222s 3s/step - loss: 1.3344 - accuracy: 0.5294 - val_loss: 1.3587 - val_accuracy: 0.5618\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 2s - loss: 1.3479 - accuracy: 0.5199\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.3390 to: 1.3378\n",
      "64/63 [==============================] - 222s 3s/step - loss: 1.3474 - accuracy: 0.5186 - val_loss: 1.3378 - val_accuracy: 0.5588\n",
      "Alex_Net - Loading previous training data\n",
      "Alex_Net - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_Net.hdf5\n",
      "Alex_Net - Loaded best val_loss value: 1.3378 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_Net_val_loss.txt\n",
      "VGG_16 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5965 - accuracy: 0.4363\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.4996 to: 1.4929\n",
      "64/63 [==============================] - 352s 5s/step - loss: 1.5946 - accuracy: 0.4373 - val_loss: 1.4929 - val_accuracy: 0.4706\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5851 - accuracy: 0.4343\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.4929 to: 1.4897\n",
      "64/63 [==============================] - 366s 6s/step - loss: 1.5813 - accuracy: 0.4363 - val_loss: 1.4897 - val_accuracy: 0.4676\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5519 - accuracy: 0.4432\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 353s 6s/step - loss: 1.5522 - accuracy: 0.4422 - val_loss: 1.5627 - val_accuracy: 0.4618\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5385 - accuracy: 0.4552\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 336s 5s/step - loss: 1.5415 - accuracy: 0.4569 - val_loss: 1.5826 - val_accuracy: 0.4176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5785 - accuracy: 0.4343\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 344s 5s/step - loss: 1.5747 - accuracy: 0.4353 - val_loss: 1.5619 - val_accuracy: 0.4206\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5529 - accuracy: 0.4512\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 356s 6s/step - loss: 1.5525 - accuracy: 0.4520 - val_loss: 1.6258 - val_accuracy: 0.4235\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5780 - accuracy: 0.4283\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 363s 6s/step - loss: 1.5767 - accuracy: 0.4304 - val_loss: 1.5204 - val_accuracy: 0.4412\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5045 - accuracy: 0.4691\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 364s 6s/step - loss: 1.5060 - accuracy: 0.4686 - val_loss: 1.5680 - val_accuracy: 0.4500\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5126 - accuracy: 0.4602\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 344s 5s/step - loss: 1.5188 - accuracy: 0.4569 - val_loss: 1.5227 - val_accuracy: 0.4618\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5365 - accuracy: 0.4502\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 347s 5s/step - loss: 1.5332 - accuracy: 0.4510 - val_loss: 1.5297 - val_accuracy: 0.4735\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5513 - accuracy: 0.4492\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 346s 5s/step - loss: 1.5496 - accuracy: 0.4490 - val_loss: 1.5678 - val_accuracy: 0.4529\n",
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5428 - accuracy: 0.4622\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 343s 5s/step - loss: 1.5477 - accuracy: 0.4578 - val_loss: 1.5801 - val_accuracy: 0.4265\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5074 - accuracy: 0.4741\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 344s 5s/step - loss: 1.5167 - accuracy: 0.4706 - val_loss: 1.7369 - val_accuracy: 0.4382\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.4863 - accuracy: 0.4771\n",
      "VGG_16 Val_loss did not improve. Best is: 1.4897. Not Caching\n",
      "64/63 [==============================] - 348s 5s/step - loss: 1.4923 - accuracy: 0.4745 - val_loss: 1.5074 - val_accuracy: 0.4529\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 3s - loss: 1.5025 - accuracy: 0.4771\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.4897 to: 1.4810\n",
      "64/63 [==============================] - 347s 5s/step - loss: 1.5022 - accuracy: 0.4765 - val_loss: 1.4810 - val_accuracy: 0.5059\n",
      "VGG_16 - Loading previous training data\n",
      "VGG_16 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16.hdf5\n",
      "VGG_16 - Loaded best val_loss value: 1.4810 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16_val_loss.txt\n",
      "Case_0 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9367 - accuracy: 0.6922\n",
      "Case_0 Val_loss did not improve. Best is: 1.0161. Not Caching\n",
      "64/63 [==============================] - 62s 976ms/step - loss: 0.9321 - accuracy: 0.6941 - val_loss: 1.0330 - val_accuracy: 0.6500\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6982\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0161 to: 1.0161\n",
      "64/63 [==============================] - 54s 846ms/step - loss: 0.9115 - accuracy: 0.6990 - val_loss: 1.0161 - val_accuracy: 0.6441\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8850 - accuracy: 0.7022\n",
      "Case_0 Val_loss did not improve. Best is: 1.0161. Not Caching\n",
      "64/63 [==============================] - 52s 812ms/step - loss: 0.8844 - accuracy: 0.7020 - val_loss: 1.0243 - val_accuracy: 0.6441\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8877 - accuracy: 0.7122\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0161 to: 1.0154\n",
      "64/63 [==============================] - 56s 868ms/step - loss: 0.8862 - accuracy: 0.7137 - val_loss: 1.0154 - val_accuracy: 0.6441\n",
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8846 - accuracy: 0.7153\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0154 to: 1.0153\n",
      "64/63 [==============================] - 60s 936ms/step - loss: 0.8799 - accuracy: 0.7176 - val_loss: 1.0153 - val_accuracy: 0.6471\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8899 - accuracy: 0.7052\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0153 to: 1.0105\n",
      "64/63 [==============================] - 56s 878ms/step - loss: 0.8912 - accuracy: 0.7049 - val_loss: 1.0105 - val_accuracy: 0.6500\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8692 - accuracy: 0.7122\n",
      "Case_0 Val_loss did not improve. Best is: 1.0105. Not Caching\n",
      "64/63 [==============================] - 52s 805ms/step - loss: 0.8713 - accuracy: 0.7118 - val_loss: 1.0206 - val_accuracy: 0.6529\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8959 - accuracy: 0.7032\n",
      "Case_0 Val_loss did not improve. Best is: 1.0105. Not Caching\n",
      "64/63 [==============================] - 52s 814ms/step - loss: 0.8911 - accuracy: 0.7069 - val_loss: 1.0136 - val_accuracy: 0.6412\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8603 - accuracy: 0.7201\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0105 to: 1.0103\n",
      "64/63 [==============================] - 55s 859ms/step - loss: 0.8625 - accuracy: 0.7206 - val_loss: 1.0103 - val_accuracy: 0.6324\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8544 - accuracy: 0.7221\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0103 to: 1.0100\n",
      "64/63 [==============================] - 56s 869ms/step - loss: 0.8555 - accuracy: 0.7216 - val_loss: 1.0100 - val_accuracy: 0.6471\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8529 - accuracy: 0.7131\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0100 to: 1.0066\n",
      "64/63 [==============================] - 55s 866ms/step - loss: 0.8498 - accuracy: 0.7147 - val_loss: 1.0066 - val_accuracy: 0.6324\n",
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8533 - accuracy: 0.7201\n",
      "Case_0 Val_loss did not improve. Best is: 1.0066. Not Caching\n",
      "64/63 [==============================] - 55s 864ms/step - loss: 0.8537 - accuracy: 0.7196 - val_loss: 1.0113 - val_accuracy: 0.6382\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8399 - accuracy: 0.7092\n",
      "Case_0 Val_loss did not improve. Best is: 1.0066. Not Caching\n",
      "64/63 [==============================] - 53s 830ms/step - loss: 0.8397 - accuracy: 0.7078 - val_loss: 1.0212 - val_accuracy: 0.6618\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8636 - accuracy: 0.7311\n",
      "Case_0 Val_loss did not improve. Best is: 1.0066. Not Caching\n",
      "64/63 [==============================] - 52s 813ms/step - loss: 0.8644 - accuracy: 0.7314 - val_loss: 1.0068 - val_accuracy: 0.6559\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8416 - accuracy: 0.7191\n",
      "Case_0 Val_loss did not improve. Best is: 1.0066. Not Caching\n",
      "64/63 [==============================] - 52s 812ms/step - loss: 0.8391 - accuracy: 0.7196 - val_loss: 1.0168 - val_accuracy: 0.6559\n",
      "Case_0 - Loading previous training data\n",
      "Case_0 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0.hdf5\n",
      "Case_0 - Loaded best val_loss value: 1.0066 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0_val_loss.txt\n",
      "Case_1 - Training for 15 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9595 - accuracy: 0.6793\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0186 to: 1.0175\n",
      "64/63 [==============================] - 48s 749ms/step - loss: 0.9563 - accuracy: 0.6804 - val_loss: 1.0175 - val_accuracy: 0.6618\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9452 - accuracy: 0.6912\n",
      "Case_1 Val_loss did not improve. Best is: 1.0175. Not Caching\n",
      "64/63 [==============================] - 43s 667ms/step - loss: 0.9486 - accuracy: 0.6902 - val_loss: 1.0206 - val_accuracy: 0.6618\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9562 - accuracy: 0.6863\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0175 to: 1.0083\n",
      "64/63 [==============================] - 39s 616ms/step - loss: 0.9552 - accuracy: 0.6863 - val_loss: 1.0083 - val_accuracy: 0.6647\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.7022\n",
      "Case_1 Val_loss did not improve. Best is: 1.0083. Not Caching\n",
      "64/63 [==============================] - 39s 603ms/step - loss: 0.9215 - accuracy: 0.7029 - val_loss: 1.0112 - val_accuracy: 0.6529\n",
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.6922\n",
      "Case_1 Val_loss did not improve. Best is: 1.0083. Not Caching\n",
      "64/63 [==============================] - 39s 615ms/step - loss: 0.9292 - accuracy: 0.6902 - val_loss: 1.0167 - val_accuracy: 0.6647\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.7002\n",
      "Case_1 Val_loss did not improve. Best is: 1.0083. Not Caching\n",
      "64/63 [==============================] - 38s 601ms/step - loss: 0.9098 - accuracy: 0.7010 - val_loss: 1.0289 - val_accuracy: 0.6529\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9289 - accuracy: 0.6912\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0083 to: 1.0082\n",
      "64/63 [==============================] - 39s 616ms/step - loss: 0.9308 - accuracy: 0.6922 - val_loss: 1.0082 - val_accuracy: 0.6735\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9117 - accuracy: 0.6942\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0082 to: 1.0052\n",
      "64/63 [==============================] - 41s 645ms/step - loss: 0.9150 - accuracy: 0.6922 - val_loss: 1.0052 - val_accuracy: 0.6735\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.9065 - accuracy: 0.7012\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0052 to: 0.9960\n",
      "64/63 [==============================] - 46s 726ms/step - loss: 0.9034 - accuracy: 0.7020 - val_loss: 0.9960 - val_accuracy: 0.6735\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8883 - accuracy: 0.7072\n",
      "Case_1 Val_loss did not improve. Best is: 0.9960. Not Caching\n",
      "64/63 [==============================] - 36s 558ms/step - loss: 0.8904 - accuracy: 0.7078 - val_loss: 1.0078 - val_accuracy: 0.6676\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8898 - accuracy: 0.7102\n",
      "Case_1 Val_loss did not improve. Best is: 0.9960. Not Caching\n",
      "64/63 [==============================] - 35s 549ms/step - loss: 0.8865 - accuracy: 0.7098 - val_loss: 1.0088 - val_accuracy: 0.6559\n",
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8736 - accuracy: 0.7092\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 0.9960 to: 0.9909\n",
      "64/63 [==============================] - 40s 621ms/step - loss: 0.8842 - accuracy: 0.7059 - val_loss: 0.9909 - val_accuracy: 0.6765\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8903 - accuracy: 0.7022\n",
      "Case_1 Val_loss did not improve. Best is: 0.9909. Not Caching\n",
      "64/63 [==============================] - 38s 593ms/step - loss: 0.8952 - accuracy: 0.7020 - val_loss: 0.9937 - val_accuracy: 0.6706\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8821 - accuracy: 0.7211\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 0.9909 to: 0.9903\n",
      "64/63 [==============================] - 39s 614ms/step - loss: 0.8904 - accuracy: 0.7176 - val_loss: 0.9903 - val_accuracy: 0.6676\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 0.8753 - accuracy: 0.7112\n",
      "Case_1 Val_loss did not improve. Best is: 0.9903. Not Caching\n",
      "64/63 [==============================] - 39s 602ms/step - loss: 0.8728 - accuracy: 0.7127 - val_loss: 0.9920 - val_accuracy: 0.6706\n",
      "Case_1 - Loading previous training data\n",
      "Case_1 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1.hdf5\n",
      "Case_1 - Loaded best val_loss value: 0.9903 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1_val_loss.txt\n",
      "Case_2 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4484 - accuracy: 0.5169\n",
      "Case_2 Val_loss did not improve. Best is: 1.2793. Not Caching\n",
      "64/63 [==============================] - 43s 677ms/step - loss: 1.4438 - accuracy: 0.5196 - val_loss: 1.2849 - val_accuracy: 0.5706\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4222 - accuracy: 0.5309\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.2793 to: 1.2764\n",
      "64/63 [==============================] - 38s 592ms/step - loss: 1.4142 - accuracy: 0.5353 - val_loss: 1.2764 - val_accuracy: 0.5882\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4088 - accuracy: 0.5369\n",
      "Case_2 Val_loss did not improve. Best is: 1.2764. Not Caching\n",
      "64/63 [==============================] - 35s 539ms/step - loss: 1.4106 - accuracy: 0.5363 - val_loss: 1.2998 - val_accuracy: 0.5588\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4029 - accuracy: 0.5269\n",
      "Case_2 Val_loss did not improve. Best is: 1.2764. Not Caching\n",
      "64/63 [==============================] - 37s 578ms/step - loss: 1.3997 - accuracy: 0.5265 - val_loss: 1.2931 - val_accuracy: 0.5794\n",
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4087 - accuracy: 0.5169\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.2764 to: 1.2661\n",
      "64/63 [==============================] - 36s 570ms/step - loss: 1.4086 - accuracy: 0.5186 - val_loss: 1.2661 - val_accuracy: 0.6000\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4137 - accuracy: 0.5159\n",
      "Case_2 Val_loss did not improve. Best is: 1.2661. Not Caching\n",
      "64/63 [==============================] - 36s 566ms/step - loss: 1.4113 - accuracy: 0.5196 - val_loss: 1.2772 - val_accuracy: 0.5941\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3647 - accuracy: 0.5518\n",
      "Case_2 Val_loss did not improve. Best is: 1.2661. Not Caching\n",
      "64/63 [==============================] - 36s 565ms/step - loss: 1.3708 - accuracy: 0.5510 - val_loss: 1.2740 - val_accuracy: 0.5853\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4203 - accuracy: 0.5239\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.2661 to: 1.2607\n",
      "64/63 [==============================] - 36s 565ms/step - loss: 1.4193 - accuracy: 0.5235 - val_loss: 1.2607 - val_accuracy: 0.5882\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3609 - accuracy: 0.5558\n",
      "Case_2 Val_loss did not improve. Best is: 1.2607. Not Caching\n",
      "64/63 [==============================] - 38s 593ms/step - loss: 1.3641 - accuracy: 0.5549 - val_loss: 1.2862 - val_accuracy: 0.5824\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3858 - accuracy: 0.5259\n",
      "Case_2 Val_loss did not improve. Best is: 1.2607. Not Caching\n",
      "64/63 [==============================] - 39s 609ms/step - loss: 1.3842 - accuracy: 0.5275 - val_loss: 1.2678 - val_accuracy: 0.6000\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4077 - accuracy: 0.5369\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.2607 to: 1.2529\n",
      "64/63 [==============================] - 37s 576ms/step - loss: 1.4129 - accuracy: 0.5373 - val_loss: 1.2529 - val_accuracy: 0.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.4115 - accuracy: 0.5269\n",
      "Case_2 Val_loss did not improve. Best is: 1.2529. Not Caching\n",
      "64/63 [==============================] - 36s 561ms/step - loss: 1.4085 - accuracy: 0.5265 - val_loss: 1.2763 - val_accuracy: 0.5941\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3837 - accuracy: 0.5458\n",
      "Case_2 Val_loss did not improve. Best is: 1.2529. Not Caching\n",
      "64/63 [==============================] - 36s 560ms/step - loss: 1.3869 - accuracy: 0.5422 - val_loss: 1.2624 - val_accuracy: 0.5912\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3802 - accuracy: 0.5488\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.2529 to: 1.2525\n",
      "64/63 [==============================] - 38s 591ms/step - loss: 1.3819 - accuracy: 0.5461 - val_loss: 1.2525 - val_accuracy: 0.6118\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.3327 - accuracy: 0.5618\n",
      "Case_2 Val_loss did not improve. Best is: 1.2525. Not Caching\n",
      "64/63 [==============================] - 38s 588ms/step - loss: 1.3367 - accuracy: 0.5608 - val_loss: 1.2623 - val_accuracy: 0.5853\n",
      "Case_2 - Loading previous training data\n",
      "Case_2 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2.hdf5\n",
      "Case_2 - Loaded best val_loss value: 1.2525 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2_val_loss.txt\n",
      "Case_3 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 63.75 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1957 - accuracy: 0.5767\n",
      "Case_3 Val_loss did not improve. Best is: 1.2905. Not Caching\n",
      "64/63 [==============================] - 41s 635ms/step - loss: 1.1938 - accuracy: 0.5765 - val_loss: 1.3050 - val_accuracy: 0.5588\n",
      "Epoch 2/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1867 - accuracy: 0.5747\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.2905 to: 1.2763\n",
      "64/63 [==============================] - 38s 588ms/step - loss: 1.1823 - accuracy: 0.5775 - val_loss: 1.2763 - val_accuracy: 0.5853\n",
      "Epoch 3/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1852 - accuracy: 0.5867\n",
      "Case_3 Val_loss did not improve. Best is: 1.2763. Not Caching\n",
      "64/63 [==============================] - 39s 614ms/step - loss: 1.1845 - accuracy: 0.5863 - val_loss: 1.2831 - val_accuracy: 0.5735\n",
      "Epoch 4/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1712 - accuracy: 0.5936\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.2763 to: 1.2750\n",
      "64/63 [==============================] - 36s 556ms/step - loss: 1.1671 - accuracy: 0.5961 - val_loss: 1.2750 - val_accuracy: 0.5735\n",
      "Epoch 5/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1588 - accuracy: 0.5976\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.2750 to: 1.2641\n",
      "64/63 [==============================] - 35s 554ms/step - loss: 1.1585 - accuracy: 0.5971 - val_loss: 1.2641 - val_accuracy: 0.5853\n",
      "Epoch 6/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1325 - accuracy: 0.6026\n",
      "Case_3 Val_loss did not improve. Best is: 1.2641. Not Caching\n",
      "64/63 [==============================] - 35s 542ms/step - loss: 1.1294 - accuracy: 0.6049 - val_loss: 1.2812 - val_accuracy: 0.5735\n",
      "Epoch 7/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1467 - accuracy: 0.5876\n",
      "Case_3 Val_loss did not improve. Best is: 1.2641. Not Caching\n",
      "64/63 [==============================] - 35s 545ms/step - loss: 1.1483 - accuracy: 0.5863 - val_loss: 1.2760 - val_accuracy: 0.5853\n",
      "Epoch 8/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1410 - accuracy: 0.6046\n",
      "Case_3 Val_loss did not improve. Best is: 1.2641. Not Caching\n",
      "64/63 [==============================] - 35s 553ms/step - loss: 1.1425 - accuracy: 0.6010 - val_loss: 1.2748 - val_accuracy: 0.5559\n",
      "Epoch 9/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1362 - accuracy: 0.5926\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.2641 to: 1.2481\n",
      "64/63 [==============================] - 35s 552ms/step - loss: 1.1360 - accuracy: 0.5931 - val_loss: 1.2481 - val_accuracy: 0.5882\n",
      "Epoch 10/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1108 - accuracy: 0.6125\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 37s 571ms/step - loss: 1.1231 - accuracy: 0.6088 - val_loss: 1.2682 - val_accuracy: 0.5882\n",
      "Epoch 11/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.0908 - accuracy: 0.6125\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 38s 587ms/step - loss: 1.0902 - accuracy: 0.6118 - val_loss: 1.2778 - val_accuracy: 0.5706\n",
      "Epoch 12/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1213 - accuracy: 0.5996\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 38s 589ms/step - loss: 1.1190 - accuracy: 0.6010 - val_loss: 1.2551 - val_accuracy: 0.5794\n",
      "Epoch 13/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1056 - accuracy: 0.6145\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 35s 553ms/step - loss: 1.1119 - accuracy: 0.6108 - val_loss: 1.2536 - val_accuracy: 0.5765\n",
      "Epoch 14/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1044 - accuracy: 0.6036\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 36s 558ms/step - loss: 1.1030 - accuracy: 0.6049 - val_loss: 1.2592 - val_accuracy: 0.5824\n",
      "Epoch 15/15\n",
      "63/63 [============================>.] - ETA: 0s - loss: 1.1205 - accuracy: 0.6036\n",
      "Case_3 Val_loss did not improve. Best is: 1.2481. Not Caching\n",
      "64/63 [==============================] - 36s 557ms/step - loss: 1.1198 - accuracy: 0.6049 - val_loss: 1.2631 - val_accuracy: 0.5618\n",
      "Case_3 - Loading previous training data\n",
      "Case_3 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3.hdf5\n",
      "Case_3 - Loaded best val_loss value: 1.2481 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3_val_loss.txt\n",
      "Training CNN architectures complete\n",
      "Gathering final results\n",
      "Predicting completed. Final Ensemble Loss: 1.0402 Final Ensemble Accuracy: 68.82%\n",
      "Individual models loss and accuracy:\n",
      "Alex_net Loss: 1.3378 Accuracy: 55.88%\n",
      "VGG_16_net Loss: 1.4810 Accuracy: 50.59%\n",
      "Case_0_net Loss: 1.0066 Accuracy: 63.24%\n",
      "Case_1_net Loss: 0.9903 Accuracy: 66.76%\n",
      "Case_2_net Loss: 1.2525 Accuracy: 61.18%\n",
      "Case_3_net Loss: 1.2481 Accuracy: 58.82%\n"
     ]
    }
   ],
   "source": [
    "# Create Ensemble method\n",
    "# This setup allows incremental training\n",
    "\n",
    "SHOULD_TRAIN = True         # true if models should be trained\n",
    "TRAIN_FOR_NUM_EPOCHS = 15   # the number of epochs to run if training\n",
    "LOAD_WEIGHTS = True         # set to false to restart training\n",
    "\n",
    "\n",
    "\n",
    "# 1. Create the used architectures. This will also load the cached best model if exists\n",
    "print(\"Initialising CNN architectures\")\n",
    "alex_net   = alex_net_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "vgg_16_net = vgg_16_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "case_0_net = case_0_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "case_1_net = case_1_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "case_2_net = case_2_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "case_3_net = case_3_CNN_augmented_checkpointed(LOAD_WEIGHTS)\n",
    "\n",
    "\n",
    "if SHOULD_TRAIN:\n",
    "  # each model will be trained here. Once training is complete, the checkpointed weights\n",
    "  # are loaded back in. This is to allow the final tests to be done on the best performing model\n",
    "  print(\"Training CNN architectures for {} epochs\".format(TRAIN_FOR_NUM_EPOCHS))\n",
    "  alex_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  alex_net.load_checkpointed_weights()\n",
    "  vgg_16_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  vgg_16_net.load_checkpointed_weights()\n",
    "  case_0_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_0_net.load_checkpointed_weights()\n",
    "  case_1_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_1_net.load_checkpointed_weights()\n",
    "  case_2_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_2_net.load_checkpointed_weights()\n",
    "  case_3_net.train(trainX, trainY, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_3_net.load_checkpointed_weights()\n",
    "  print(\"Training CNN architectures complete\")\n",
    "\n",
    "print(\"Gathering final results\")\n",
    "\n",
    "prediction_a = alex_net.predict(testX)\n",
    "mult_a = alex_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "prediction_v = vgg_16_net.predict(testX)\n",
    "mult_v = vgg_16_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "prediction_1 = case_1_net.predict(testX)\n",
    "mult_1 = case_1_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "prediction_2 = case_2_net.predict(testX)\n",
    "mult_2 = case_2_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "prediction_3 = case_3_net.predict(testX)\n",
    "mult_3 = case_3_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "prediction_0 = case_0_net.predict(testX)\n",
    "mult_0 = case_0_net.get_accuracy_multiplier(trainX, trainY)\n",
    "\n",
    "# Normalize the multiplier\n",
    "sum_mult = mult_a + mult_v + mult_0 + mult_1 + mult_2 + mult_3\n",
    "mult_a /= sum_mult\n",
    "mult_v /= sum_mult\n",
    "mult_0 /= sum_mult\n",
    "mult_1 /= sum_mult\n",
    "mult_2 /= sum_mult\n",
    "mult_3 /= sum_mult\n",
    "\n",
    "total_predictions = (prediction_a * mult_a) + (prediction_v * mult_v) + (prediction_0 * mult_0) + (prediction_1 * mult_1) + (prediction_2 * mult_2) + (prediction_3 * mult_3)\n",
    "\n",
    "final_loss, final_accuracy = get_prediction_result(total_predictions, testY)\n",
    "\n",
    "print(\"Predicting completed. Final Ensemble Loss: {:.4f} Final Ensemble Accuracy: {:.2f}%\".format(final_loss, final_accuracy*100.0))\n",
    "\n",
    "# Print Individual algorithm accuracy\n",
    "alex_net_loss, alex_net_accuracy          = get_prediction_result(prediction_a, testY)\n",
    "vgg_16_net_loss, vgg_16_net_accuracy      = get_prediction_result(prediction_v, testY)\n",
    "case_0_net_loss, case_0_net_accuracy      = get_prediction_result(prediction_0, testY)\n",
    "case_1_net_loss, case_1_net_accuracy      = get_prediction_result(prediction_1, testY)\n",
    "case_2_net_loss, case_2_net_accuracy      = get_prediction_result(prediction_2, testY)\n",
    "case_3_net_loss, case_3_net_accuracy      = get_prediction_result(prediction_3, testY)\n",
    "\n",
    "print(\"Individual models loss and accuracy:\")\n",
    "print(\"Alex_net Loss: {:.4f} Accuracy: {:.2f}%\".format(alex_net_loss, alex_net_accuracy*100.0))\n",
    "print(\"VGG_16_net Loss: {:.4f} Accuracy: {:.2f}%\".format(vgg_16_net_loss, vgg_16_net_accuracy*100.0))\n",
    "print(\"Case_0_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_0_net_loss, case_0_net_accuracy*100.0))\n",
    "print(\"Case_1_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_1_net_loss, case_1_net_accuracy*100.0))\n",
    "print(\"Case_2_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_2_net_loss, case_2_net_accuracy*100.0))\n",
    "print(\"Case_3_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_3_net_loss, case_3_net_accuracy*100.0))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PartA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
