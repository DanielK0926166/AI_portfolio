{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Deep Learning - Convolutional Network Ensemble (Bagging)</b></center><br>\n",
    "\n",
    "The code below implements multiple convolutional networks and utilises them in an ensemble to improve prediction. The ensemble method used here is Bootstrap Aggregation or Bagging. Bagging affects how the data is selected for training. Each model only receives a subset of the training data which are randomly selected with replacement. The effectiveness of this method comes from the fact that each model will be trained on different datasets. However, this method is not optimal when the training data is limited like it is in this case.<br> <br>\n",
    "The dataset is very small, it contains 17 categories but only 1020 images in total. To adjust for this, data augmentation is implemented also the data is only split into 'training' and 'validation' sets, 'test' sets are not being utilised.<br> <br>\n",
    "Data checkpointing is also used that caches the best model with the smallest validation loss, this allows incremental training. Custom data checkpointing was used to compensate for a shortcoming of Keras' built-in data checkpointer as that system loses the information about the validation loss when loading the data back in. This custom checkpointer saves the validation loss into a separate file.<br>\n",
    "<br>\n",
    "As can be seen on the results, the ensemble method resulted in a better performance than any of the architectures did by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "Ho5hTcQaHgNq",
    "outputId": "5bc145da-8017-4822-f71b-f4ce7f4c0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Lt_ZAF3vIm0T",
    "outputId": "0d3e629d-c1fc-4bb1-cb9d-983f34334667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 128, 128, 3) (1020,)\n",
      "(340, 128, 128, 3) (340,)\n"
     ]
    }
   ],
   "source": [
    "# Handle dataset\n",
    "FILE_PATH = os.getcwd()\n",
    "CACHE_PATH = FILE_PATH+\"/cached/deeplearning/\"\n",
    "data_file = FILE_PATH+\"/data/DeepLearning/conv_net_data.zip\"\n",
    "\n",
    "data = zipfile.ZipFile(data_file)\n",
    "data_file = data.open('data1.h5')\n",
    "\n",
    "def loadDataH5():\n",
    "    with h5py.File(data_file,'r') as hf:\n",
    "      trainX = np.array(hf.get('trainX'))\n",
    "      trainY = np.array(hf.get('trainY'))\n",
    "      valX = np.array(hf.get('valX'))\n",
    "      valY = np.array(hf.get('valY'))\n",
    "      print (trainX.shape,trainY.shape)\n",
    "      print (valX.shape,valY.shape)\n",
    "      return trainX, trainY, valX, valY \n",
    " \n",
    "trainX, trainY, testX, testY = loadDataH5() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vKSuUaOSJl_"
   },
   "outputs": [],
   "source": [
    "# const variables about the dataset\n",
    "IMG_DEPTH   = 3\n",
    "IMG_WIDTH   = 128\n",
    "IMG_HEIGHT  = 128\n",
    "NUM_CLASSES = 17\n",
    "\n",
    "\n",
    "NUM_EPOCHS  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHJLcMHljA6r"
   },
   "outputs": [],
   "source": [
    "# All Architectures used\n",
    "\n",
    "def create_case_0_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    # add conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_1_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_2_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_3_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 4\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_alex_net_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(96, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 5\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_vgg_16_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "    model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vmrahc9WQQ4H"
   },
   "outputs": [],
   "source": [
    "# Base class for all architectures to derive from. Handles checkpointing and data augmentation\n",
    "class CNN_base_augmented_checkpointed:\n",
    "  def __init__(self, try_to_load_weights=True, ext_name=\"\"):\n",
    "    # Initialise Variables\n",
    "    self.BATCH_SIZE = 16\n",
    "  \n",
    "    # Initialise model\n",
    "    self.model = self.create_model(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH, NUM_CLASSES)\n",
    "    self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "    # Saved data file names\n",
    "    self.weights_file_name = \"{}/{}{}.hdf5\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "    self.val_loss_file_name = \"{}/{}{}_val_loss.txt\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "\n",
    "    # The current best val_loss\n",
    "    self.best_val_loss = float(\"inf\")\n",
    "    if try_to_load_weights:\n",
    "      self.load_checkpointed_weights()\n",
    "    \n",
    "\n",
    "  def load_best_val_loss_value(self):\n",
    "    \"\"\"\n",
    "    This function is used to load the validation loss of the best model.\n",
    "    It is stored in a txt file when the best model's weights were cached\n",
    "    \"\"\"\n",
    "    if os.path.isfile(self.val_loss_file_name):\n",
    "      loss_file = open(self.val_loss_file_name, \"r\")\n",
    "      self.best_val_loss = float(loss_file.read())\n",
    "      loss_file.close()\n",
    "      print(\"{} - Loaded best val_loss value: {:.4f} from: {}\".format(self.get_name(), self.best_val_loss, self.val_loss_file_name))\n",
    "\n",
    "\n",
    "  def save_weights_callback(self, logs):\n",
    "    \"\"\"\n",
    "    This is a custom callback function to be used to cache the weights of the network when val_loss decreases\n",
    "    \"\"\"\n",
    "    val_loss = logs.get('val_loss')\n",
    "    if val_loss < self.best_val_loss:\n",
    "      print(\"\\n{} - Caching Checkpoint - val_loss has improved from: {:.4f} to: {:.4f}\".format(self.get_name(), self.best_val_loss, val_loss))\n",
    "      self.best_val_loss = val_loss\n",
    "      \n",
    "      self.model.save_weights(self.weights_file_name)\n",
    "\n",
    "      loss_file = open(self.val_loss_file_name,\"w\")\n",
    "      loss_file.write(str(val_loss))\n",
    "      loss_file.close()\n",
    "    else:\n",
    "      print(\"\\n{} Val_loss did not improve. Best is: {:.4f}. Not Caching\".format(self.get_name(), self.best_val_loss))\n",
    "\n",
    "  def train(self, tr_x, tr_y, val_x, val_y, num_epochs=NUM_EPOCHS):\n",
    "    print(\"{} - Training for {} epochs\".format(self.get_name(), num_epochs))\n",
    "\n",
    "    # Create the image generator for Data Augmentation\n",
    "    datagen_batch = len(tr_x) / self.BATCH_SIZE\n",
    "    datagen = self.create_image_generator()\n",
    "\n",
    "    # Create the model checkpointer\n",
    "    custom_checkpointing_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs:self.save_weights_callback(logs))\n",
    "\n",
    "    # Train\n",
    "    self.model.fit(datagen.flow(tr_x, tr_y, batch_size=self.BATCH_SIZE), steps_per_epoch=datagen_batch, epochs=num_epochs, validation_data=(val_x, val_y), callbacks=[custom_checkpointing_callback])\n",
    "\n",
    "  def load_checkpointed_weights(self):\n",
    "    # If possible load the previous training results. Weights and assosiated val_loss\n",
    "    if os.path.isfile(self.weights_file_name):\n",
    "      print(\"{} - Loading previous training data\".format(self.get_name()))\n",
    "      self.model.load_weights(self.weights_file_name)\n",
    "      print(\"{} - Loaded previous model weights from: {}\".format(self.get_name(), self.weights_file_name))\n",
    "      self.load_best_val_loss_value()\n",
    "\n",
    "  def predict(self, val_x):\n",
    "    return self.model.predict(val_x)\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def create_image_generator(self):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_name(self):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xatn1Sk4Skob"
   },
   "outputs": [],
   "source": [
    "# Similar to Alex-net\n",
    "class alex_net_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Alex_Net\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=40,\n",
    "       width_shift_range=0.2,\n",
    "       height_shift_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_alex_net_architecture(width, height, depth, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEkI4BBIVa15"
   },
   "outputs": [],
   "source": [
    "# Similar to VGG 16\n",
    "class vgg_16_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"VGG_16\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=30,\n",
    "       width_shift_range=0.3,\n",
    "       height_shift_range=0.3,\n",
    "       horizontal_flip=False,\n",
    "       vertical_flip=True)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_vgg_16_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 0\n",
    "class case_0_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_0\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=20,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_0_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5taidIni1IY"
   },
   "outputs": [],
   "source": [
    "# Case 1 \n",
    "class case_1_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_1\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=50,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_1_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7teNYHJk7tA"
   },
   "outputs": [],
   "source": [
    "# Case 2\n",
    "class case_2_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_2\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=75,\n",
    "       width_shift_range=0.4,\n",
    "       height_shift_range=0.4,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.1)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_2_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekEkWY3xlwoV"
   },
   "outputs": [],
   "source": [
    "# Case 3\n",
    "class case_3_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_3\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=10,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.3)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_3_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGKmAV-Fy6K2"
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate loss and accuracy\n",
    "def get_prediction_result(predicted_results, y_labels):\n",
    "  # Calculate Loss\n",
    "  y_labels_hot = np_utils.to_categorical(y_labels, NUM_CLASSES)\n",
    "  predicted_results_clipped = tf.clip_by_value(predicted_results, 1e-10, 1.0)\n",
    "  loss = (1.0/predicted_results_clipped.shape[0]) * tf.math.reduce_sum(-tf.math.multiply(tf.math.log(predicted_results_clipped),y_labels_hot))\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  predicted_y = tf.math.argmax(predicted_results, 1)\n",
    "  match_array = tf.equal(predicted_y, y_labels)\n",
    "  accuracy = tf.reduce_sum(tf.cast(match_array, tf.float32)) / len(y_labels)\n",
    "\n",
    "  return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "sBPwEPF6zGK6",
    "outputId": "1dae2dd6-9170-4e6a-bd0a-d7531fccaa88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CNN architectures\n",
      "Alex_Net - Loading previous training data\n",
      "Alex_Net - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBAG.hdf5\n",
      "Alex_Net - Loaded best val_loss value: 1.1057 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBAG_val_loss.txt\n",
      "VGG_16 - Loading previous training data\n",
      "VGG_16 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BAG.hdf5\n",
      "VGG_16 - Loaded best val_loss value: 1.8587 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BAG_val_loss.txt\n",
      "Case_0 - Loading previous training data\n",
      "Case_0 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BAG.hdf5\n",
      "Case_0 - Loaded best val_loss value: 1.0632 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BAG_val_loss.txt\n",
      "Case_1 - Loading previous training data\n",
      "Case_1 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BAG.hdf5\n",
      "Case_1 - Loaded best val_loss value: 1.0799 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BAG_val_loss.txt\n",
      "Case_2 - Loading previous training data\n",
      "Case_2 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BAG.hdf5\n",
      "Case_2 - Loaded best val_loss value: 1.1539 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BAG_val_loss.txt\n",
      "Case_3 - Loading previous training data\n",
      "Case_3 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BAG.hdf5\n",
      "Case_3 - Loaded best val_loss value: 1.3988 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BAG_val_loss.txt\n",
      "Training CNN architectures for 15 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex_Net - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2482 - accuracy: 0.5676\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 198s 4s/step - loss: 1.2494 - accuracy: 0.5686 - val_loss: 1.1651 - val_accuracy: 0.6088\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1716 - accuracy: 0.6087\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 198s 4s/step - loss: 1.1794 - accuracy: 0.6055 - val_loss: 1.3479 - val_accuracy: 0.5500\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1197 - accuracy: 0.6181\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 189s 3s/step - loss: 1.1220 - accuracy: 0.6136 - val_loss: 1.3444 - val_accuracy: 0.5559\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0493 - accuracy: 0.6392\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 192s 3s/step - loss: 1.0546 - accuracy: 0.6367 - val_loss: 1.4009 - val_accuracy: 0.5353\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0564 - accuracy: 0.6122\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 194s 4s/step - loss: 1.0627 - accuracy: 0.6078 - val_loss: 1.2922 - val_accuracy: 0.5588\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0427 - accuracy: 0.6357\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 193s 4s/step - loss: 1.0404 - accuracy: 0.6367 - val_loss: 1.1446 - val_accuracy: 0.5941\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0035 - accuracy: 0.6392\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 197s 4s/step - loss: 1.0028 - accuracy: 0.6390 - val_loss: 1.2834 - val_accuracy: 0.5971\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0718 - accuracy: 0.6416\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 190s 3s/step - loss: 1.0681 - accuracy: 0.6413 - val_loss: 1.2113 - val_accuracy: 0.5735\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9439 - accuracy: 0.6651\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 229s 4s/step - loss: 0.9429 - accuracy: 0.6655 - val_loss: 1.2338 - val_accuracy: 0.5853\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6651\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 208s 4s/step - loss: 0.9237 - accuracy: 0.6644 - val_loss: 1.2823 - val_accuracy: 0.5941\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8979 - accuracy: 0.6968\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 198s 4s/step - loss: 0.9061 - accuracy: 0.6932 - val_loss: 1.4762 - val_accuracy: 0.5294\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9522 - accuracy: 0.6757\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 196s 4s/step - loss: 0.9485 - accuracy: 0.6770 - val_loss: 1.3413 - val_accuracy: 0.5853\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9802 - accuracy: 0.6522\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 197s 4s/step - loss: 0.9778 - accuracy: 0.6517 - val_loss: 1.2182 - val_accuracy: 0.6176\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7976 - accuracy: 0.6992\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 200s 4s/step - loss: 0.8062 - accuracy: 0.6955 - val_loss: 1.2865 - val_accuracy: 0.6206\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8406 - accuracy: 0.6957\n",
      "Alex_Net Val_loss did not improve. Best is: 1.1057. Not Caching\n",
      "55/54 [==============================] - 199s 4s/step - loss: 0.8412 - accuracy: 0.6943 - val_loss: 1.2811 - val_accuracy: 0.5588\n",
      "Alex_Net - Loading previous training data\n",
      "Alex_Net - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBAG.hdf5\n",
      "Alex_Net - Loaded best val_loss value: 1.1057 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBAG_val_loss.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_16 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.9616 - accuracy: 0.3126\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8587. Not Caching\n",
      "55/54 [==============================] - 263s 5s/step - loss: 1.9583 - accuracy: 0.3137 - val_loss: 2.0248 - val_accuracy: 0.3353\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.9326 - accuracy: 0.3149\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8587. Not Caching\n",
      "55/54 [==============================] - 272s 5s/step - loss: 1.9372 - accuracy: 0.3137 - val_loss: 1.9381 - val_accuracy: 0.3206\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.9157 - accuracy: 0.3102\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8587. Not Caching\n",
      "55/54 [==============================] - 277s 5s/step - loss: 1.9216 - accuracy: 0.3103 - val_loss: 1.9685 - val_accuracy: 0.3471\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.8845 - accuracy: 0.3396\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.8587 to: 1.8099\n",
      "55/54 [==============================] - 268s 5s/step - loss: 1.8834 - accuracy: 0.3368 - val_loss: 1.8099 - val_accuracy: 0.3529\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.8736 - accuracy: 0.3184\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8099. Not Caching\n",
      "55/54 [==============================] - 260s 5s/step - loss: 1.8721 - accuracy: 0.3218 - val_loss: 1.8186 - val_accuracy: 0.3588\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.8246 - accuracy: 0.3584\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8099. Not Caching\n",
      "55/54 [==============================] - 270s 5s/step - loss: 1.8236 - accuracy: 0.3587 - val_loss: 1.9885 - val_accuracy: 0.2941\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.8361 - accuracy: 0.3537\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8099. Not Caching\n",
      "55/54 [==============================] - 255s 5s/step - loss: 1.8309 - accuracy: 0.3552 - val_loss: 1.9763 - val_accuracy: 0.3324\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.8409 - accuracy: 0.3243\n",
      "VGG_16 Val_loss did not improve. Best is: 1.8099. Not Caching\n",
      "55/54 [==============================] - 289s 5s/step - loss: 1.8353 - accuracy: 0.3299 - val_loss: 1.8157 - val_accuracy: 0.3529\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 1s - loss: 1.8131 - accuracy: 0.3349\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.8099 to: 1.7135\n",
      "55/54 [==============================] - 356s 6s/step - loss: 1.8155 - accuracy: 0.3333 - val_loss: 1.7135 - val_accuracy: 0.4147\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 1s - loss: 1.9427 - accuracy: 0.3302\n",
      "VGG_16 Val_loss did not improve. Best is: 1.7135. Not Caching\n",
      "55/54 [==============================] - 367s 7s/step - loss: 1.9349 - accuracy: 0.3322 - val_loss: 1.8676 - val_accuracy: 0.3412\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.7409 - accuracy: 0.3549\n",
      "VGG_16 Val_loss did not improve. Best is: 1.7135. Not Caching\n",
      "55/54 [==============================] - 284s 5s/step - loss: 1.7413 - accuracy: 0.3541 - val_loss: 1.7493 - val_accuracy: 0.3853\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.7628 - accuracy: 0.3690\n",
      "VGG_16 Val_loss did not improve. Best is: 1.7135. Not Caching\n",
      "55/54 [==============================] - 261s 5s/step - loss: 1.7629 - accuracy: 0.3668 - val_loss: 1.9175 - val_accuracy: 0.3441\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.7595 - accuracy: 0.3655\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.7135 to: 1.6886\n",
      "55/54 [==============================] - 282s 5s/step - loss: 1.7573 - accuracy: 0.3702 - val_loss: 1.6886 - val_accuracy: 0.3912\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.7244 - accuracy: 0.3760\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 1.6886 to: 1.6809\n",
      "55/54 [==============================] - 277s 5s/step - loss: 1.7216 - accuracy: 0.3749 - val_loss: 1.6809 - val_accuracy: 0.4147\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.7289 - accuracy: 0.3984\n",
      "VGG_16 Val_loss did not improve. Best is: 1.6809. Not Caching\n",
      "55/54 [==============================] - 281s 5s/step - loss: 1.7257 - accuracy: 0.3979 - val_loss: 1.7099 - val_accuracy: 0.4441\n",
      "VGG_16 - Loading previous training data\n",
      "VGG_16 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BAG.hdf5\n",
      "VGG_16 - Loaded best val_loss value: 1.6809 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BAG_val_loss.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:39: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:40: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case_0 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9293 - accuracy: 0.6792\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0632 to: 1.0503\n",
      "55/54 [==============================] - 55s 1s/step - loss: 0.9264 - accuracy: 0.6817 - val_loss: 1.0503 - val_accuracy: 0.6559\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8404 - accuracy: 0.7133\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.0503 to: 0.9958\n",
      "55/54 [==============================] - 49s 900ms/step - loss: 0.8384 - accuracy: 0.7140 - val_loss: 0.9958 - val_accuracy: 0.6882\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7816 - accuracy: 0.7215\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 41s 751ms/step - loss: 0.7820 - accuracy: 0.7220 - val_loss: 1.0726 - val_accuracy: 0.6412\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7770 - accuracy: 0.7368\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 44s 808ms/step - loss: 0.7739 - accuracy: 0.7393 - val_loss: 1.0634 - val_accuracy: 0.6676\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7190 - accuracy: 0.7720\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 45s 817ms/step - loss: 0.7201 - accuracy: 0.7716 - val_loss: 1.0859 - val_accuracy: 0.6559\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6833 - accuracy: 0.7697\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 43s 779ms/step - loss: 0.6865 - accuracy: 0.7647 - val_loss: 1.1485 - val_accuracy: 0.6235\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6201 - accuracy: 0.7944\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 41s 749ms/step - loss: 0.6242 - accuracy: 0.7947 - val_loss: 1.0535 - val_accuracy: 0.6471\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6626 - accuracy: 0.7638\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 41s 743ms/step - loss: 0.6604 - accuracy: 0.7647 - val_loss: 1.1505 - val_accuracy: 0.6206\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.7897\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 45s 819ms/step - loss: 0.5930 - accuracy: 0.7901 - val_loss: 1.1523 - val_accuracy: 0.6382\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.8167\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 42s 767ms/step - loss: 0.5395 - accuracy: 0.8166 - val_loss: 1.1969 - val_accuracy: 0.6412\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.8190\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 42s 768ms/step - loss: 0.5497 - accuracy: 0.8166 - val_loss: 1.1300 - val_accuracy: 0.6471\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5503 - accuracy: 0.8320\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 42s 760ms/step - loss: 0.5454 - accuracy: 0.8339 - val_loss: 1.1675 - val_accuracy: 0.6294\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.8179\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 42s 765ms/step - loss: 0.5460 - accuracy: 0.8166 - val_loss: 1.1610 - val_accuracy: 0.6765\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.8249\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 40s 730ms/step - loss: 0.5256 - accuracy: 0.8258 - val_loss: 1.2049 - val_accuracy: 0.6382\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.8202\n",
      "Case_0 Val_loss did not improve. Best is: 0.9958. Not Caching\n",
      "55/54 [==============================] - 42s 759ms/step - loss: 0.5205 - accuracy: 0.8189 - val_loss: 1.1309 - val_accuracy: 0.6882\n",
      "Case_0 - Loading previous training data\n",
      "Case_0 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BAG.hdf5\n",
      "Case_0 - Loaded best val_loss value: 0.9958 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BAG_val_loss.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case_1 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0590 - accuracy: 0.6298\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0799 to: 1.0659\n",
      "55/54 [==============================] - 36s 646ms/step - loss: 1.0637 - accuracy: 0.6286 - val_loss: 1.0659 - val_accuracy: 0.6529\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9803 - accuracy: 0.6710\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.0659 to: 1.0504\n",
      "55/54 [==============================] - 31s 564ms/step - loss: 1.0029 - accuracy: 0.6644 - val_loss: 1.0504 - val_accuracy: 0.6412\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9356 - accuracy: 0.6663\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 32s 579ms/step - loss: 0.9342 - accuracy: 0.6667 - val_loss: 1.1815 - val_accuracy: 0.6206\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.6992\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 32s 591ms/step - loss: 0.9180 - accuracy: 0.7047 - val_loss: 1.0581 - val_accuracy: 0.6382\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8574 - accuracy: 0.7133\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 33s 594ms/step - loss: 0.8571 - accuracy: 0.7105 - val_loss: 1.1692 - val_accuracy: 0.6265\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8355 - accuracy: 0.7133\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 34s 621ms/step - loss: 0.8331 - accuracy: 0.7128 - val_loss: 1.3194 - val_accuracy: 0.5853\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.8328 - accuracy: 0.7168\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 31s 570ms/step - loss: 0.8287 - accuracy: 0.7197 - val_loss: 1.1515 - val_accuracy: 0.6147\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7896 - accuracy: 0.7168\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 31s 570ms/step - loss: 0.7933 - accuracy: 0.7128 - val_loss: 1.2370 - val_accuracy: 0.6059\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7545 - accuracy: 0.7391\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 31s 566ms/step - loss: 0.7566 - accuracy: 0.7370 - val_loss: 1.2170 - val_accuracy: 0.6324\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.7329 - accuracy: 0.7438\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 32s 574ms/step - loss: 0.7323 - accuracy: 0.7439 - val_loss: 1.1471 - val_accuracy: 0.6529\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6492 - accuracy: 0.7908\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 31s 569ms/step - loss: 0.6586 - accuracy: 0.7878 - val_loss: 1.0926 - val_accuracy: 0.6941\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6365 - accuracy: 0.7791\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 33s 594ms/step - loss: 0.6412 - accuracy: 0.7785 - val_loss: 1.2583 - val_accuracy: 0.6412\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6559 - accuracy: 0.7603\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 31s 565ms/step - loss: 0.6536 - accuracy: 0.7589 - val_loss: 1.1756 - val_accuracy: 0.6500\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6374 - accuracy: 0.7861\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 33s 596ms/step - loss: 0.6337 - accuracy: 0.7855 - val_loss: 1.2015 - val_accuracy: 0.6265\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.7814\n",
      "Case_1 Val_loss did not improve. Best is: 1.0504. Not Caching\n",
      "55/54 [==============================] - 33s 609ms/step - loss: 0.6650 - accuracy: 0.7820 - val_loss: 1.2306 - val_accuracy: 0.6559\n",
      "Case_1 - Loading previous training data\n",
      "Case_1 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BAG.hdf5\n",
      "Case_1 - Loaded best val_loss value: 1.0504 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BAG_val_loss.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:51: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case_2 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.3467 - accuracy: 0.5523\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 34s 617ms/step - loss: 1.3423 - accuracy: 0.5502 - val_loss: 1.1557 - val_accuracy: 0.6059\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2666 - accuracy: 0.5523\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 30s 549ms/step - loss: 1.2626 - accuracy: 0.5536 - val_loss: 1.2281 - val_accuracy: 0.5676\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2386 - accuracy: 0.5723\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 31s 573ms/step - loss: 1.2366 - accuracy: 0.5744 - val_loss: 1.2194 - val_accuracy: 0.6088\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1532 - accuracy: 0.6146\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 31s 557ms/step - loss: 1.1525 - accuracy: 0.6148 - val_loss: 1.4468 - val_accuracy: 0.5206\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2502 - accuracy: 0.5699\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 30s 542ms/step - loss: 1.2447 - accuracy: 0.5721 - val_loss: 1.1884 - val_accuracy: 0.5941\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1851 - accuracy: 0.5993\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 30s 550ms/step - loss: 1.1790 - accuracy: 0.6032 - val_loss: 1.2226 - val_accuracy: 0.5824\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1071 - accuracy: 0.6169\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 29s 531ms/step - loss: 1.1052 - accuracy: 0.6171 - val_loss: 1.2071 - val_accuracy: 0.6147\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1468 - accuracy: 0.6087\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 31s 566ms/step - loss: 1.1479 - accuracy: 0.6078 - val_loss: 1.2957 - val_accuracy: 0.5647\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0946 - accuracy: 0.6193\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 29s 532ms/step - loss: 1.1006 - accuracy: 0.6148 - val_loss: 1.2505 - val_accuracy: 0.5824\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1260 - accuracy: 0.6052\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 31s 569ms/step - loss: 1.1206 - accuracy: 0.6078 - val_loss: 1.5257 - val_accuracy: 0.5676\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0812 - accuracy: 0.6099\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 32s 575ms/step - loss: 1.0792 - accuracy: 0.6101 - val_loss: 1.3065 - val_accuracy: 0.5559\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0764 - accuracy: 0.6263\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 31s 565ms/step - loss: 1.0824 - accuracy: 0.6240 - val_loss: 1.6818 - val_accuracy: 0.4971\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0956 - accuracy: 0.6157\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 29s 520ms/step - loss: 1.0892 - accuracy: 0.6194 - val_loss: 1.3676 - val_accuracy: 0.5735\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0682 - accuracy: 0.6216\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 28s 517ms/step - loss: 1.0820 - accuracy: 0.6159 - val_loss: 1.3189 - val_accuracy: 0.5647\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0860 - accuracy: 0.6146\n",
      "Case_2 Val_loss did not improve. Best is: 1.1539. Not Caching\n",
      "55/54 [==============================] - 30s 538ms/step - loss: 1.0760 - accuracy: 0.6194 - val_loss: 1.2382 - val_accuracy: 0.6206\n",
      "Case_2 - Loading previous training data\n",
      "Case_2 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BAG.hdf5\n",
      "Case_2 - Loaded best val_loss value: 1.1539 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BAG_val_loss.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:57: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case_3 - Training for 15 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:58: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 54.1875 steps, validate on 340 samples\n",
      "Epoch 1/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.4489 - accuracy: 0.4912\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 34s 616ms/step - loss: 1.4446 - accuracy: 0.4937 - val_loss: 1.4281 - val_accuracy: 0.4853\n",
      "Epoch 2/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.3607 - accuracy: 0.5206\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 30s 537ms/step - loss: 1.3601 - accuracy: 0.5213 - val_loss: 1.4959 - val_accuracy: 0.5059\n",
      "Epoch 3/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2967 - accuracy: 0.5253\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 29s 534ms/step - loss: 1.3046 - accuracy: 0.5236 - val_loss: 1.5952 - val_accuracy: 0.4529\n",
      "Epoch 4/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2486 - accuracy: 0.5441\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 29s 533ms/step - loss: 1.2513 - accuracy: 0.5421 - val_loss: 1.4294 - val_accuracy: 0.5206\n",
      "Epoch 5/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2592 - accuracy: 0.5394\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 30s 551ms/step - loss: 1.2595 - accuracy: 0.5375 - val_loss: 1.5578 - val_accuracy: 0.4765\n",
      "Epoch 6/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2089 - accuracy: 0.5652\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 28s 502ms/step - loss: 1.2123 - accuracy: 0.5652 - val_loss: 1.5382 - val_accuracy: 0.4853\n",
      "Epoch 7/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.2009 - accuracy: 0.5758\n",
      "Case_3 Val_loss did not improve. Best is: 1.3988. Not Caching\n",
      "55/54 [==============================] - 28s 502ms/step - loss: 1.1928 - accuracy: 0.5779 - val_loss: 1.6370 - val_accuracy: 0.4794\n",
      "Epoch 8/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1575 - accuracy: 0.5911\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.3988 to: 1.3714\n",
      "55/54 [==============================] - 28s 507ms/step - loss: 1.1696 - accuracy: 0.5859 - val_loss: 1.3714 - val_accuracy: 0.5353\n",
      "Epoch 9/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.1458 - accuracy: 0.5938\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 28s 502ms/step - loss: 1.1313 - accuracy: 0.5952 - val_loss: 1.5279 - val_accuracy: 0.5147\n",
      "Epoch 10/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0568 - accuracy: 0.6122\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 31s 571ms/step - loss: 1.0628 - accuracy: 0.6101 - val_loss: 1.5601 - val_accuracy: 0.5265\n",
      "Epoch 11/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0859 - accuracy: 0.6040\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 32s 580ms/step - loss: 1.0966 - accuracy: 0.6021 - val_loss: 1.6524 - val_accuracy: 0.4441\n",
      "Epoch 12/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0061 - accuracy: 0.6428\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 28s 509ms/step - loss: 1.0061 - accuracy: 0.6424 - val_loss: 1.4450 - val_accuracy: 0.5059\n",
      "Epoch 13/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0134 - accuracy: 0.6298\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 29s 529ms/step - loss: 1.0126 - accuracy: 0.6275 - val_loss: 1.5559 - val_accuracy: 0.5353\n",
      "Epoch 14/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 1.0010 - accuracy: 0.6369\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 28s 503ms/step - loss: 1.0045 - accuracy: 0.6344 - val_loss: 1.4325 - val_accuracy: 0.4941\n",
      "Epoch 15/15\n",
      "54/54 [============================>.] - ETA: 0s - loss: 0.9549 - accuracy: 0.6569\n",
      "Case_3 Val_loss did not improve. Best is: 1.3714. Not Caching\n",
      "55/54 [==============================] - 31s 573ms/step - loss: 0.9563 - accuracy: 0.6551 - val_loss: 1.4037 - val_accuracy: 0.5559\n",
      "Case_3 - Loading previous training data\n",
      "Case_3 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BAG.hdf5\n",
      "Case_3 - Loaded best val_loss value: 1.3714 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BAG_val_loss.txt\n",
      "Training CNN architectures complete\n",
      "Gathering final results\n",
      "Predicting completed. Final Ensemble Loss: 0.0915 Final Ensemble Accuracy: 70.29%\n",
      "Alex_net Loss: 1.1057 Accuracy: 63.24%\n",
      "VGG_16_net Loss: 1.6809 Accuracy: 41.47%\n",
      "Case_0_net Loss: 0.9958 Accuracy: 68.82%\n",
      "Case_1_net Loss: 1.0504 Accuracy: 64.12%\n",
      "Case_2_net Loss: 1.1539 Accuracy: 61.47%\n",
      "Case_3_net Loss: 1.3714 Accuracy: 53.53%\n"
     ]
    }
   ],
   "source": [
    "# Create Ensemble method\n",
    "# This setup allows incremental training\n",
    "\n",
    "SHOULD_TRAIN = True         # true if models should be trained\n",
    "TRAIN_FOR_NUM_EPOCHS = 15   # the number of epochs to run if training\n",
    "LOAD_WEIGHTS = True         # set to false to restart training\n",
    "\n",
    "SPLIT_RATIO = 0.85          # the bag size used relative to the whole dataset\n",
    "\n",
    "# 1. Create the used architectures. This will also load the cached best model if exists\n",
    "print(\"Initialising CNN architectures\")\n",
    "alex_net   = alex_net_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "vgg_16_net = vgg_16_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "case_0_net = case_0_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "case_1_net = case_1_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "case_2_net = case_2_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "case_3_net = case_3_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BAG\")\n",
    "\n",
    "if SHOULD_TRAIN:\n",
    "  data_size = len(trainX)\n",
    "  size = int(data_size * SPLIT_RATIO)\n",
    "\n",
    "  # each model will be trained here. Once training is complete, the checkpointed weights\n",
    "  # are loaded back in. This is to allow the final tests to be done on the best performing model\n",
    "  print(\"Training CNN architectures for {} epochs\".format(TRAIN_FOR_NUM_EPOCHS))\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  alex_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  alex_net.load_checkpointed_weights()\n",
    "\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  vgg_16_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  vgg_16_net.load_checkpointed_weights()\n",
    "\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  case_0_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_0_net.load_checkpointed_weights()\n",
    "\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  case_1_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_1_net.load_checkpointed_weights()\n",
    "\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  case_2_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_2_net.load_checkpointed_weights()\n",
    "\n",
    "  indices = np.random.choice(data_size, size, replace=True)\n",
    "  split_data_x = trainX[[indices]]\n",
    "  split_data_y = trainY[[indices]]\n",
    "  case_3_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  case_3_net.load_checkpointed_weights()\n",
    "\n",
    "  print(\"Training CNN architectures complete\")\n",
    "\n",
    "print(\"Gathering final results\")\n",
    "\n",
    "prediction_a = alex_net.predict(testX)\n",
    "prediction_v = vgg_16_net.predict(testX)\n",
    "prediction_0 = case_0_net.predict(testX)\n",
    "prediction_1 = case_1_net.predict(testX)\n",
    "prediction_2 = case_2_net.predict(testX)\n",
    "prediction_3 = case_3_net.predict(testX)\n",
    "\n",
    "total_predictions = prediction_a + prediction_v + prediction_0 + prediction_1 + prediction_2 + prediction_3\n",
    "\n",
    "final_loss, final_accuracy = get_prediction_result(total_predictions, testY)\n",
    "\n",
    "print(\"Predicting completed. Final Ensemble Loss: {:.4f} Final Ensemble Accuracy: {:.2f}%\".format(final_loss, final_accuracy*100.0))\n",
    "\n",
    "# Print Individual algorithm accuracy\n",
    "alex_net_loss, alex_net_accuracy          = get_prediction_result(prediction_a, testY)\n",
    "vgg_16_net_loss, vgg_16_net_accuracy      = get_prediction_result(prediction_v, testY)\n",
    "case_0_net_loss, case_0_net_accuracy      = get_prediction_result(prediction_0, testY)\n",
    "case_1_net_loss, case_1_net_accuracy      = get_prediction_result(prediction_1, testY)\n",
    "case_2_net_loss, case_2_net_accuracy      = get_prediction_result(prediction_2, testY)\n",
    "case_3_net_loss, case_3_net_accuracy      = get_prediction_result(prediction_3, testY)\n",
    "\n",
    "print(\"Alex_net Loss: {:.4f} Accuracy: {:.2f}%\".format(alex_net_loss, alex_net_accuracy*100.0))\n",
    "print(\"VGG_16_net Loss: {:.4f} Accuracy: {:.2f}%\".format(vgg_16_net_loss, vgg_16_net_accuracy*100.0))\n",
    "print(\"Case_0_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_0_net_loss, case_0_net_accuracy*100.0))\n",
    "print(\"Case_1_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_1_net_loss, case_1_net_accuracy*100.0))\n",
    "print(\"Case_2_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_2_net_loss, case_2_net_accuracy*100.0))\n",
    "print(\"Case_3_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_3_net_loss, case_3_net_accuracy*100.0))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PartA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
