{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Deep Learning - Convolutional Network Ensemble (Boosting)</b></center><br>\n",
    "\n",
    "The code below implements multiple convolutional networks and utilises them in an ensemble to improve prediction. The ensemble method used here is Boosting. Boosting affects how the data is selected for training. Each model only receives a subset of the training data. The first model to be trained gets a random selection with replacement of the data. The following models receive the training data that couldn't be correctly predicted by the previous model and some randomly selected data with replacement. The effectiveness of this method comes from the fact that each incorrectly predicted training data instance will be retrained on other models until a correct prediction is found. <br> <br>\n",
    "The dataset is very small, it contains 17 categories but only 1020 images in total. To adjust for this, data augmentation is implemented also the data is only split into 'training' and 'validation' sets, 'test' sets are not being utilised.<br> <br>\n",
    "Data checkpointing is also used that caches the best model with the smallest validation loss, this allows incremental training. Custom data checkpointing was used to compensate for a shortcoming of Keras' built-in data checkpointer as that system loses the information about the validation loss when loading the data back in. This custom checkpointer saves the validation loss into a separate file.<br>\n",
    "<br>\n",
    "As can be seen on the results, the ensemble method resulted in a better performance than any of the architectures did by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "Ho5hTcQaHgNq",
    "outputId": "5bc145da-8017-4822-f71b-f4ce7f4c0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Lt_ZAF3vIm0T",
    "outputId": "0d3e629d-c1fc-4bb1-cb9d-983f34334667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 128, 128, 3) (1020,)\n",
      "(340, 128, 128, 3) (340,)\n"
     ]
    }
   ],
   "source": [
    "# Handle dataset\n",
    "FILE_PATH = os.getcwd()\n",
    "CACHE_PATH = FILE_PATH+\"/cached/deeplearning/\"\n",
    "data_file = FILE_PATH+\"/data/DeepLearning/conv_net_data.zip\"\n",
    "\n",
    "data = zipfile.ZipFile(data_file)\n",
    "data_file = data.open('data1.h5')\n",
    "\n",
    "def loadDataH5():\n",
    "    with h5py.File(data_file,'r') as hf:\n",
    "      trainX = np.array(hf.get('trainX'))\n",
    "      trainY = np.array(hf.get('trainY'))\n",
    "      valX = np.array(hf.get('valX'))\n",
    "      valY = np.array(hf.get('valY'))\n",
    "      print (trainX.shape,trainY.shape)\n",
    "      print (valX.shape,valY.shape)\n",
    "      return trainX, trainY, valX, valY \n",
    " \n",
    "trainX, trainY, testX, testY = loadDataH5() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vKSuUaOSJl_"
   },
   "outputs": [],
   "source": [
    "# const variables about the dataset\n",
    "IMG_DEPTH   = 3\n",
    "IMG_WIDTH   = 128\n",
    "IMG_HEIGHT  = 128\n",
    "NUM_CLASSES = 17\n",
    "\n",
    "\n",
    "NUM_EPOCHS  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHJLcMHljA6r"
   },
   "outputs": [],
   "source": [
    "# All Architectures used\n",
    "\n",
    "def create_case_0_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    # add conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_1_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_2_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_case_3_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 4\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_alex_net_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(96, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    # add pooling layer 1\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 2\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    # add conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add conv layer 5\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    # add pooling layer 3\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_vgg_16_architecture(width, height, depth, classes):\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 2 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add 3 conv layers\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation='relu'))\n",
    "\n",
    "    # add pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add fully connected layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vmrahc9WQQ4H"
   },
   "outputs": [],
   "source": [
    "# Base class for all architectures to derive from. Handles checkpointing and data augmentation\n",
    "class CNN_base_augmented_checkpointed:\n",
    "  def __init__(self, try_to_load_weights=True, ext_name=\"\"):\n",
    "    # Initialise Variables\n",
    "    self.BATCH_SIZE = 16\n",
    "  \n",
    "    # Initialise model\n",
    "    self.model = self.create_model(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH, NUM_CLASSES)\n",
    "    self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "    # Saved data file names\n",
    "    self.weights_file_name = \"{}/{}{}.hdf5\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "    self.val_loss_file_name = \"{}/{}{}_val_loss.txt\".format(CACHE_PATH, self.get_name(), ext_name)\n",
    "\n",
    "    # The current best val_loss\n",
    "    self.best_val_loss = float(\"inf\")\n",
    "    if try_to_load_weights:\n",
    "      self.load_checkpointed_weights()\n",
    "    \n",
    "\n",
    "  def load_best_val_loss_value(self):\n",
    "    \"\"\"\n",
    "    This function is used to load the validation loss of the best model.\n",
    "    It is stored in a txt file when the best model's weights were cached\n",
    "    \"\"\"\n",
    "    if os.path.isfile(self.val_loss_file_name):\n",
    "      loss_file = open(self.val_loss_file_name, \"r\")\n",
    "      self.best_val_loss = float(loss_file.read())\n",
    "      loss_file.close()\n",
    "      print(\"{} - Loaded best val_loss value: {:.4f} from: {}\".format(self.get_name(), self.best_val_loss, self.val_loss_file_name))\n",
    "\n",
    "\n",
    "  def save_weights_callback(self, logs):\n",
    "    \"\"\"\n",
    "    This is a custom callback function to be used to cache the weights of the network when val_loss decreases\n",
    "    \"\"\"\n",
    "    val_loss = logs.get('val_loss')\n",
    "    if val_loss < self.best_val_loss:\n",
    "      print(\"\\n{} - Caching Checkpoint - val_loss has improved from: {:.4f} to: {:.4f}\".format(self.get_name(), self.best_val_loss, val_loss))\n",
    "      self.best_val_loss = val_loss\n",
    "      \n",
    "      self.model.save_weights(self.weights_file_name)\n",
    "\n",
    "      loss_file = open(self.val_loss_file_name,\"w\")\n",
    "      loss_file.write(str(val_loss))\n",
    "      loss_file.close()\n",
    "    else:\n",
    "      print(\"\\n{} Val_loss did not improve. Best is: {:.4f}. Not Caching\".format(self.get_name(), self.best_val_loss))\n",
    "\n",
    "  def train(self, tr_x, tr_y, val_x, val_y, num_epochs=NUM_EPOCHS):\n",
    "    print(\"{} - Training for {} epochs\".format(self.get_name(), num_epochs))\n",
    "\n",
    "    # Create the image generator for Data Augmentation\n",
    "    datagen_batch = int(len(tr_x) / self.BATCH_SIZE)\n",
    "    datagen = self.create_image_generator()\n",
    "\n",
    "    # Create the model checkpointer\n",
    "    custom_checkpointing_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs:self.save_weights_callback(logs))\n",
    "\n",
    "    # Train\n",
    "    self.model.fit(datagen.flow(tr_x, tr_y, batch_size=self.BATCH_SIZE), steps_per_epoch=datagen_batch, epochs=num_epochs, validation_data=(val_x, val_y), callbacks=[custom_checkpointing_callback])\n",
    "\n",
    "  def load_checkpointed_weights(self):\n",
    "    # If possible load the previous training results. Weights and assosiated val_loss\n",
    "    if os.path.isfile(self.weights_file_name):\n",
    "      print(\"{} - Loading previous training data\".format(self.get_name()))\n",
    "      self.model.load_weights(self.weights_file_name)\n",
    "      print(\"{} - Loaded previous model weights from: {}\".format(self.get_name(), self.weights_file_name))\n",
    "      self.load_best_val_loss_value()\n",
    "\n",
    "  def predict(self, val_x):\n",
    "    return self.model.predict(val_x)\n",
    "\n",
    "  def get_instances_incorrectly_predicted(self, tr_x, tr_y):\n",
    "    # Get Prediction\n",
    "    pred_y = self.predict(tr_x)\n",
    "    # Calculate Accuracy\n",
    "    pred_y = tf.math.argmax(pred_y, 1)\n",
    "    match_array = tf.not_equal(pred_y, tr_y)\n",
    "    indices = tf.where(match_array)\n",
    "    return tf.transpose(indices)[0]\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def create_image_generator(self):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_name(self):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xatn1Sk4Skob"
   },
   "outputs": [],
   "source": [
    "# Similar to Alex-net\n",
    "class alex_net_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Alex_Net\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=40,\n",
    "       width_shift_range=0.2,\n",
    "       height_shift_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_alex_net_architecture(width, height, depth, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEkI4BBIVa15"
   },
   "outputs": [],
   "source": [
    "# Similar to VGG 16\n",
    "class vgg_16_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"VGG_16\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=30,\n",
    "       width_shift_range=0.2,\n",
    "       height_shift_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_vgg_16_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 0\n",
    "class case_0_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_0\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=20,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_0_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5taidIni1IY"
   },
   "outputs": [],
   "source": [
    "# Case 1 \n",
    "class case_1_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_1\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=50,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_1_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7teNYHJk7tA"
   },
   "outputs": [],
   "source": [
    "# Case 2\n",
    "class case_2_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_2\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=75,\n",
    "       width_shift_range=0.4,\n",
    "       height_shift_range=0.4,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.1)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_2_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekEkWY3xlwoV"
   },
   "outputs": [],
   "source": [
    "# Case 3\n",
    "class case_3_CNN_augmented_checkpointed(CNN_base_augmented_checkpointed):\n",
    "  def get_name(self):\n",
    "    return \"Case_3\"\n",
    "\n",
    "  def create_image_generator(self):\n",
    "   return tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range=10,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       vertical_flip=False,\n",
    "       zoom_range=0.3)\n",
    "\n",
    "\n",
    "  def create_model(self, width, height, depth, classes):\n",
    "    return create_case_3_architecture(width, height, depth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGKmAV-Fy6K2"
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate loss and accuracy\n",
    "def get_prediction_result(predicted_results, y_labels):\n",
    "  # Calculate Loss\n",
    "  y_labels_hot = np_utils.to_categorical(y_labels, NUM_CLASSES)\n",
    "  predicted_results_clipped = tf.clip_by_value(predicted_results, 1e-10, 1.0)\n",
    "  loss = (1.0/predicted_results_clipped.shape[0]) * tf.math.reduce_sum(-tf.math.multiply(tf.math.log(predicted_results_clipped),y_labels_hot))\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  predicted_y = tf.math.argmax(predicted_results, 1)\n",
    "  match_array = tf.equal(predicted_y, y_labels)\n",
    "  accuracy = tf.reduce_sum(tf.cast(match_array, tf.float32)) / len(y_labels)\n",
    "\n",
    "  return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YBeSiBhOnoGQ",
    "outputId": "45b523e3-af0d-47d2-cb33-237480b4ef3a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CNN architectures\n",
      "Training CNN architectures for 30 epochs\n",
      "Alex_Net will train on 867 data instances\n",
      "Alex_Net - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8320 - accuracy: 0.0719\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: inf to: 2.8280\n",
      "54/54 [==============================] - 220s 4s/step - loss: 2.8319 - accuracy: 0.0717 - val_loss: 2.8280 - val_accuracy: 0.0765\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 2s - loss: 2.8229 - accuracy: 0.0898\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.8280 to: 2.8138\n",
      "54/54 [==============================] - 180s 3s/step - loss: 2.8228 - accuracy: 0.0905 - val_loss: 2.8138 - val_accuracy: 0.1147\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.7952 - accuracy: 0.1293\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.8138 to: 2.7618\n",
      "54/54 [==============================] - 186s 3s/step - loss: 2.7945 - accuracy: 0.1281 - val_loss: 2.7618 - val_accuracy: 0.1000\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.6693 - accuracy: 0.1401\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.7618 to: 2.4271\n",
      "54/54 [==============================] - 211s 4s/step - loss: 2.6674 - accuracy: 0.1398 - val_loss: 2.4271 - val_accuracy: 0.1735\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.4925 - accuracy: 0.1509\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.4271 to: 2.2936\n",
      "54/54 [==============================] - 224s 4s/step - loss: 2.4912 - accuracy: 0.1504 - val_loss: 2.2936 - val_accuracy: 0.2059\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.3858 - accuracy: 0.1772\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.2936 to: 2.1644\n",
      "54/54 [==============================] - 209s 4s/step - loss: 2.3863 - accuracy: 0.1786 - val_loss: 2.1644 - val_accuracy: 0.2618\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.3116 - accuracy: 0.1892\n",
      "Alex_Net Val_loss did not improve. Best is: 2.1644. Not Caching\n",
      "54/54 [==============================] - 190s 4s/step - loss: 2.3117 - accuracy: 0.1868 - val_loss: 2.1708 - val_accuracy: 0.2206\n",
      "Epoch 8/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.1881 - accuracy: 0.2383\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.1644 to: 2.1522\n",
      "54/54 [==============================] - 190s 4s/step - loss: 2.2062 - accuracy: 0.2338 - val_loss: 2.1522 - val_accuracy: 0.2647\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.1732 - accuracy: 0.2359\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 2.1522 to: 1.9974\n",
      "54/54 [==============================] - 191s 4s/step - loss: 2.1714 - accuracy: 0.2362 - val_loss: 1.9974 - val_accuracy: 0.2794\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.1651 - accuracy: 0.2551\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.9974 to: 1.9409\n",
      "54/54 [==============================] - 193s 4s/step - loss: 2.1625 - accuracy: 0.2550 - val_loss: 1.9409 - val_accuracy: 0.2971\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.0635 - accuracy: 0.2539\n",
      "Alex_Net Val_loss did not improve. Best is: 1.9409. Not Caching\n",
      "54/54 [==============================] - 192s 4s/step - loss: 2.0814 - accuracy: 0.2491 - val_loss: 2.0982 - val_accuracy: 0.2500\n",
      "Epoch 12/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.0541 - accuracy: 0.2743\n",
      "Alex_Net Val_loss did not improve. Best is: 1.9409. Not Caching\n",
      "54/54 [==============================] - 192s 4s/step - loss: 2.0598 - accuracy: 0.2738 - val_loss: 2.0036 - val_accuracy: 0.2824\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.0312 - accuracy: 0.2910\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.9409 to: 1.9160\n",
      "54/54 [==============================] - 191s 4s/step - loss: 2.0345 - accuracy: 0.2914 - val_loss: 1.9160 - val_accuracy: 0.2735\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9798 - accuracy: 0.2826\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.9160 to: 1.7515\n",
      "54/54 [==============================] - 198s 4s/step - loss: 1.9762 - accuracy: 0.2844 - val_loss: 1.7515 - val_accuracy: 0.3647\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9763 - accuracy: 0.3078\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.7515 to: 1.7331\n",
      "54/54 [==============================] - 203s 4s/step - loss: 1.9748 - accuracy: 0.3067 - val_loss: 1.7331 - val_accuracy: 0.3882\n",
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9102 - accuracy: 0.3030\n",
      "Alex_Net Val_loss did not improve. Best is: 1.7331. Not Caching\n",
      "54/54 [==============================] - 199s 4s/step - loss: 1.9126 - accuracy: 0.3020 - val_loss: 1.8166 - val_accuracy: 0.3559\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9368 - accuracy: 0.3138\n",
      "Alex_Net Val_loss did not improve. Best is: 1.7331. Not Caching\n",
      "54/54 [==============================] - 192s 4s/step - loss: 1.9314 - accuracy: 0.3161 - val_loss: 1.7948 - val_accuracy: 0.3588\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9114 - accuracy: 0.3210\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.7331 to: 1.5990\n",
      "54/54 [==============================] - 193s 4s/step - loss: 1.9089 - accuracy: 0.3220 - val_loss: 1.5990 - val_accuracy: 0.4294\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.8399 - accuracy: 0.3305\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5990. Not Caching\n",
      "54/54 [==============================] - 223s 4s/step - loss: 1.8353 - accuracy: 0.3325 - val_loss: 1.6329 - val_accuracy: 0.3912\n",
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.9162 - accuracy: 0.3234\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5990. Not Caching\n",
      "54/54 [==============================] - 212s 4s/step - loss: 1.9160 - accuracy: 0.3231 - val_loss: 1.6644 - val_accuracy: 0.4000\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.8305 - accuracy: 0.3473\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5990. Not Caching\n",
      "54/54 [==============================] - 194s 4s/step - loss: 1.8282 - accuracy: 0.3502 - val_loss: 1.6419 - val_accuracy: 0.4206\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7719 - accuracy: 0.3593\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5990. Not Caching\n",
      "54/54 [==============================] - 191s 4s/step - loss: 1.7803 - accuracy: 0.3584 - val_loss: 1.7061 - val_accuracy: 0.3618\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7985 - accuracy: 0.3796\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5990. Not Caching\n",
      "54/54 [==============================] - 196s 4s/step - loss: 1.7982 - accuracy: 0.3831 - val_loss: 1.7446 - val_accuracy: 0.3882\n",
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7627 - accuracy: 0.3749\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.5990 to: 1.5813\n",
      "54/54 [==============================] - 198s 4s/step - loss: 1.7637 - accuracy: 0.3713 - val_loss: 1.5813 - val_accuracy: 0.4059\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7723 - accuracy: 0.3665\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5813. Not Caching\n",
      "54/54 [==============================] - 211s 4s/step - loss: 1.7736 - accuracy: 0.3643 - val_loss: 1.7143 - val_accuracy: 0.4029\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7479 - accuracy: 0.3557\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.5813 to: 1.5569\n",
      "54/54 [==============================] - 201s 4s/step - loss: 1.7489 - accuracy: 0.3537 - val_loss: 1.5569 - val_accuracy: 0.4265\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7239 - accuracy: 0.3940\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5569. Not Caching\n",
      "54/54 [==============================] - 192s 4s/step - loss: 1.7275 - accuracy: 0.3925 - val_loss: 1.7838 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.7121 - accuracy: 0.3832\n",
      "Alex_Net Val_loss did not improve. Best is: 1.5569. Not Caching\n",
      "54/54 [==============================] - 189s 4s/step - loss: 1.7124 - accuracy: 0.3819 - val_loss: 1.6191 - val_accuracy: 0.4118\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.6576 - accuracy: 0.4048\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.5569 to: 1.5512\n",
      "54/54 [==============================] - 191s 4s/step - loss: 1.6585 - accuracy: 0.4078 - val_loss: 1.5512 - val_accuracy: 0.4735\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 1.6237 - accuracy: 0.4132\n",
      "Alex_Net - Caching Checkpoint - val_loss has improved from: 1.5512 to: 1.4700\n",
      "54/54 [==============================] - 192s 4s/step - loss: 1.6261 - accuracy: 0.4101 - val_loss: 1.4700 - val_accuracy: 0.4559\n",
      "Alex_Net - Loading previous training data\n",
      "Alex_Net - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBOOST.hdf5\n",
      "Alex_Net - Loaded best val_loss value: 1.4700 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Alex_NetBOOST_val_loss.txt\n",
      "VGG_16 has missed 397 data instances\n",
      "Case_0 - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.5853 - accuracy: 0.1629\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: inf to: 2.3038\n",
      "54/54 [==============================] - 51s 938ms/step - loss: 2.5806 - accuracy: 0.1645 - val_loss: 2.3038 - val_accuracy: 0.2265\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.2308 - accuracy: 0.2347\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 2.3038 to: 2.0267\n",
      "54/54 [==============================] - 43s 793ms/step - loss: 2.2330 - accuracy: 0.2374 - val_loss: 2.0267 - val_accuracy: 0.3059\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0609 - accuracy: 0.2910\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 2.0267 to: 1.8658\n",
      "54/54 [==============================] - 42s 783ms/step - loss: 2.0627 - accuracy: 0.2914 - val_loss: 1.8658 - val_accuracy: 0.3529\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9093 - accuracy: 0.3545\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.8658 to: 1.6744\n",
      "54/54 [==============================] - 42s 784ms/step - loss: 1.9142 - accuracy: 0.3514 - val_loss: 1.6744 - val_accuracy: 0.4529\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8337 - accuracy: 0.3737\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.6744 to: 1.6148\n",
      "54/54 [==============================] - 43s 787ms/step - loss: 1.8285 - accuracy: 0.3749 - val_loss: 1.6148 - val_accuracy: 0.4471\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7309 - accuracy: 0.3964\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.6148 to: 1.5417\n",
      "54/54 [==============================] - 43s 793ms/step - loss: 1.7330 - accuracy: 0.3948 - val_loss: 1.5417 - val_accuracy: 0.4912\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6589 - accuracy: 0.4323\n",
      "Case_0 Val_loss did not improve. Best is: 1.5417. Not Caching\n",
      "54/54 [==============================] - 41s 757ms/step - loss: 1.6691 - accuracy: 0.4313 - val_loss: 1.6126 - val_accuracy: 0.4794\n",
      "Epoch 8/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5959 - accuracy: 0.4503\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.5417 to: 1.4013\n",
      "54/54 [==============================] - 42s 786ms/step - loss: 1.5866 - accuracy: 0.4536 - val_loss: 1.4013 - val_accuracy: 0.4912\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5216 - accuracy: 0.4671\n",
      "Case_0 Val_loss did not improve. Best is: 1.4013. Not Caching\n",
      "54/54 [==============================] - 41s 752ms/step - loss: 1.5255 - accuracy: 0.4700 - val_loss: 1.4262 - val_accuracy: 0.5147\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.4862\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.4013 to: 1.3720\n",
      "54/54 [==============================] - 42s 787ms/step - loss: 1.4972 - accuracy: 0.4888 - val_loss: 1.3720 - val_accuracy: 0.5235\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.4214 - accuracy: 0.5210\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.3720 to: 1.3289\n",
      "54/54 [==============================] - 43s 789ms/step - loss: 1.4273 - accuracy: 0.5206 - val_loss: 1.3289 - val_accuracy: 0.5588\n",
      "Epoch 12/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.4376 - accuracy: 0.5162\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.3289 to: 1.2410\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.4317 - accuracy: 0.5194 - val_loss: 1.2410 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3794 - accuracy: 0.5281\n",
      "Case_0 Val_loss did not improve. Best is: 1.2410. Not Caching\n",
      "54/54 [==============================] - 40s 749ms/step - loss: 1.3825 - accuracy: 0.5241 - val_loss: 1.2933 - val_accuracy: 0.5382\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3827 - accuracy: 0.5353\n",
      "Case_0 Val_loss did not improve. Best is: 1.2410. Not Caching\n",
      "54/54 [==============================] - 41s 751ms/step - loss: 1.3783 - accuracy: 0.5394 - val_loss: 1.2943 - val_accuracy: 0.5618\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3187 - accuracy: 0.5449\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.2410 to: 1.1893\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 1.3190 - accuracy: 0.5429 - val_loss: 1.1893 - val_accuracy: 0.6176\n",
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3058 - accuracy: 0.5461\n",
      "Case_0 Val_loss did not improve. Best is: 1.1893. Not Caching\n",
      "54/54 [==============================] - 41s 750ms/step - loss: 1.3066 - accuracy: 0.5464 - val_loss: 1.2230 - val_accuracy: 0.5941\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2388 - accuracy: 0.5605\n",
      "Case_0 Val_loss did not improve. Best is: 1.1893. Not Caching\n",
      "54/54 [==============================] - 40s 738ms/step - loss: 1.2331 - accuracy: 0.5629 - val_loss: 1.1959 - val_accuracy: 0.5853\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2095 - accuracy: 0.5844\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.1893 to: 1.1457\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 1.2068 - accuracy: 0.5828 - val_loss: 1.1457 - val_accuracy: 0.6206\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2013 - accuracy: 0.5796\n",
      "Case_0 Val_loss did not improve. Best is: 1.1457. Not Caching\n",
      "54/54 [==============================] - 41s 755ms/step - loss: 1.2011 - accuracy: 0.5781 - val_loss: 1.2527 - val_accuracy: 0.6118\n",
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1574 - accuracy: 0.5916\n",
      "Case_0 Val_loss did not improve. Best is: 1.1457. Not Caching\n",
      "54/54 [==============================] - 40s 748ms/step - loss: 1.1549 - accuracy: 0.5911 - val_loss: 1.2190 - val_accuracy: 0.5971\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1401 - accuracy: 0.5892\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.1457 to: 1.1456\n",
      "54/54 [==============================] - 43s 789ms/step - loss: 1.1412 - accuracy: 0.5875 - val_loss: 1.1456 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0910 - accuracy: 0.6156\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 41s 754ms/step - loss: 1.0892 - accuracy: 0.6169 - val_loss: 1.2696 - val_accuracy: 0.5735\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0889 - accuracy: 0.6180\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 40s 743ms/step - loss: 1.0895 - accuracy: 0.6169 - val_loss: 1.1539 - val_accuracy: 0.6059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0539 - accuracy: 0.6431\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 38s 712ms/step - loss: 1.0690 - accuracy: 0.6381 - val_loss: 1.2422 - val_accuracy: 0.5941\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.6228\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 38s 713ms/step - loss: 1.0385 - accuracy: 0.6228 - val_loss: 1.1642 - val_accuracy: 0.6265\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.6551\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 42s 785ms/step - loss: 1.0053 - accuracy: 0.6569 - val_loss: 1.1721 - val_accuracy: 0.6059\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.9750 - accuracy: 0.6479\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 41s 766ms/step - loss: 0.9801 - accuracy: 0.6463 - val_loss: 1.2484 - val_accuracy: 0.5706\n",
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.9428 - accuracy: 0.6743\n",
      "Case_0 Val_loss did not improve. Best is: 1.1456. Not Caching\n",
      "54/54 [==============================] - 41s 761ms/step - loss: 0.9400 - accuracy: 0.6757 - val_loss: 1.1504 - val_accuracy: 0.6235\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.6814\n",
      "Case_0 - Caching Checkpoint - val_loss has improved from: 1.1456 to: 1.1273\n",
      "54/54 [==============================] - 43s 796ms/step - loss: 0.9415 - accuracy: 0.6827 - val_loss: 1.1273 - val_accuracy: 0.6059\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.9325 - accuracy: 0.6707\n",
      "Case_0 Val_loss did not improve. Best is: 1.1273. Not Caching\n",
      "54/54 [==============================] - 41s 755ms/step - loss: 0.9322 - accuracy: 0.6686 - val_loss: 1.1780 - val_accuracy: 0.6088\n",
      "Case_0 - Loading previous training data\n",
      "Case_0 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BOOST.hdf5\n",
      "Case_0 - Loaded best val_loss value: 1.1273 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_0BOOST_val_loss.txt\n",
      "Case_0 has missed 179 data instances\n",
      "Case_1 - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7005 - accuracy: 0.1234\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: inf to: 2.6325\n",
      "54/54 [==============================] - 36s 662ms/step - loss: 2.7031 - accuracy: 0.1210 - val_loss: 2.6325 - val_accuracy: 0.0971\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.3943 - accuracy: 0.1952\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 2.6325 to: 2.3214\n",
      "54/54 [==============================] - 32s 588ms/step - loss: 2.3932 - accuracy: 0.1939 - val_loss: 2.3214 - val_accuracy: 0.1824\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.2290 - accuracy: 0.2204\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 2.3214 to: 2.0497\n",
      "54/54 [==============================] - 32s 591ms/step - loss: 2.2239 - accuracy: 0.2221 - val_loss: 2.0497 - val_accuracy: 0.2706\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1108 - accuracy: 0.2575\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 2.0497 to: 1.9186\n",
      "54/54 [==============================] - 32s 589ms/step - loss: 2.1005 - accuracy: 0.2632 - val_loss: 1.9186 - val_accuracy: 0.3176\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0236 - accuracy: 0.3066\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.9186 to: 1.8355\n",
      "54/54 [==============================] - 31s 577ms/step - loss: 2.0253 - accuracy: 0.3090 - val_loss: 1.8355 - val_accuracy: 0.3647\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9223 - accuracy: 0.3401\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.8355 to: 1.7033\n",
      "54/54 [==============================] - 32s 585ms/step - loss: 1.9234 - accuracy: 0.3396 - val_loss: 1.7033 - val_accuracy: 0.4088\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8382 - accuracy: 0.3605\n",
      "Case_1 Val_loss did not improve. Best is: 1.7033. Not Caching\n",
      "54/54 [==============================] - 31s 576ms/step - loss: 1.8449 - accuracy: 0.3584 - val_loss: 1.9017 - val_accuracy: 0.3765\n",
      "Epoch 8/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7786 - accuracy: 0.3808\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.7033 to: 1.4581\n",
      "54/54 [==============================] - 31s 582ms/step - loss: 1.7779 - accuracy: 0.3807 - val_loss: 1.4581 - val_accuracy: 0.5294\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7121 - accuracy: 0.4036\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.4581 to: 1.4398\n",
      "54/54 [==============================] - 32s 586ms/step - loss: 1.7100 - accuracy: 0.4054 - val_loss: 1.4398 - val_accuracy: 0.5265\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6697 - accuracy: 0.3916\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.4398 to: 1.4262\n",
      "54/54 [==============================] - 31s 582ms/step - loss: 1.6674 - accuracy: 0.3937 - val_loss: 1.4262 - val_accuracy: 0.5265\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5911 - accuracy: 0.4395\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.4262 to: 1.3858\n",
      "54/54 [==============================] - 32s 584ms/step - loss: 1.5901 - accuracy: 0.4395 - val_loss: 1.3858 - val_accuracy: 0.5353\n",
      "Epoch 12/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5607 - accuracy: 0.4623\n",
      "Case_1 Val_loss did not improve. Best is: 1.3858. Not Caching\n",
      "54/54 [==============================] - 31s 573ms/step - loss: 1.5586 - accuracy: 0.4606 - val_loss: 1.4903 - val_accuracy: 0.4706\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5079 - accuracy: 0.4671\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.3858 to: 1.3736\n",
      "54/54 [==============================] - 32s 585ms/step - loss: 1.5086 - accuracy: 0.4700 - val_loss: 1.3736 - val_accuracy: 0.5618\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.4386 - accuracy: 0.4970\n",
      "Case_1 Val_loss did not improve. Best is: 1.3736. Not Caching\n",
      "54/54 [==============================] - 31s 575ms/step - loss: 1.4341 - accuracy: 0.5006 - val_loss: 1.4076 - val_accuracy: 0.5265\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.4506 - accuracy: 0.4743\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.3736 to: 1.2084\n",
      "54/54 [==============================] - 31s 582ms/step - loss: 1.4542 - accuracy: 0.4747 - val_loss: 1.2084 - val_accuracy: 0.6088\n",
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3995 - accuracy: 0.5126\n",
      "Case_1 Val_loss did not improve. Best is: 1.2084. Not Caching\n",
      "54/54 [==============================] - 31s 577ms/step - loss: 1.4068 - accuracy: 0.5088 - val_loss: 1.2142 - val_accuracy: 0.6118\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3552 - accuracy: 0.5305\n",
      "Case_1 Val_loss did not improve. Best is: 1.2084. Not Caching\n",
      "54/54 [==============================] - 31s 567ms/step - loss: 1.3574 - accuracy: 0.5300 - val_loss: 1.2291 - val_accuracy: 0.6029\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.3388 - accuracy: 0.5293\n",
      "Case_1 Val_loss did not improve. Best is: 1.2084. Not Caching\n",
      "54/54 [==============================] - 31s 572ms/step - loss: 1.3389 - accuracy: 0.5276 - val_loss: 1.4407 - val_accuracy: 0.5088\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2662 - accuracy: 0.5461\n",
      "Case_1 Val_loss did not improve. Best is: 1.2084. Not Caching\n",
      "54/54 [==============================] - 31s 574ms/step - loss: 1.2621 - accuracy: 0.5488 - val_loss: 1.4377 - val_accuracy: 0.5118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2693 - accuracy: 0.5557\n",
      "Case_1 Val_loss did not improve. Best is: 1.2084. Not Caching\n",
      "54/54 [==============================] - 31s 575ms/step - loss: 1.2708 - accuracy: 0.5558 - val_loss: 1.2849 - val_accuracy: 0.5471\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2139 - accuracy: 0.5665\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.2084 to: 1.1485\n",
      "54/54 [==============================] - 31s 581ms/step - loss: 1.2196 - accuracy: 0.5617 - val_loss: 1.1485 - val_accuracy: 0.6441\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.2342 - accuracy: 0.5485\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.1485 to: 1.1149\n",
      "54/54 [==============================] - 32s 584ms/step - loss: 1.2398 - accuracy: 0.5476 - val_loss: 1.1149 - val_accuracy: 0.6382\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1907 - accuracy: 0.5605\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.1149 to: 1.1003\n",
      "54/54 [==============================] - 32s 588ms/step - loss: 1.1958 - accuracy: 0.5570 - val_loss: 1.1003 - val_accuracy: 0.6324\n",
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1303 - accuracy: 0.5904\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 573ms/step - loss: 1.1322 - accuracy: 0.5899 - val_loss: 1.2359 - val_accuracy: 0.5941\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1139 - accuracy: 0.5940\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 575ms/step - loss: 1.1192 - accuracy: 0.5934 - val_loss: 1.1845 - val_accuracy: 0.6059\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.1099 - accuracy: 0.6072\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 576ms/step - loss: 1.1121 - accuracy: 0.6075 - val_loss: 1.1976 - val_accuracy: 0.5765\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0746 - accuracy: 0.6299\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 572ms/step - loss: 1.0797 - accuracy: 0.6275 - val_loss: 1.2385 - val_accuracy: 0.5765\n",
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0977 - accuracy: 0.6012\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 573ms/step - loss: 1.0926 - accuracy: 0.6028 - val_loss: 1.1247 - val_accuracy: 0.6206\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.9946 - accuracy: 0.6551\n",
      "Case_1 Val_loss did not improve. Best is: 1.1003. Not Caching\n",
      "54/54 [==============================] - 31s 575ms/step - loss: 0.9942 - accuracy: 0.6569 - val_loss: 1.1211 - val_accuracy: 0.6294\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.0120 - accuracy: 0.6491\n",
      "Case_1 - Caching Checkpoint - val_loss has improved from: 1.1003 to: 1.0643\n",
      "54/54 [==============================] - 31s 581ms/step - loss: 1.0126 - accuracy: 0.6486 - val_loss: 1.0643 - val_accuracy: 0.6676\n",
      "Case_1 - Loading previous training data\n",
      "Case_1 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BOOST.hdf5\n",
      "Case_1 - Loaded best val_loss value: 1.0643 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_1BOOST_val_loss.txt\n",
      "Case_1 has missed 221 data instances\n",
      "Case_2 - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.8264 - accuracy: 0.0838\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: inf to: 2.8422\n",
      "54/54 [==============================] - 33s 606ms/step - loss: 2.8255 - accuracy: 0.0823 - val_loss: 2.8422 - val_accuracy: 0.0588\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7954 - accuracy: 0.1126\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.8422 to: 2.8133\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.7958 - accuracy: 0.1128 - val_loss: 2.8133 - val_accuracy: 0.0471\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7433 - accuracy: 0.1305\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.8133 to: 2.7296\n",
      "54/54 [==============================] - 29s 538ms/step - loss: 2.7437 - accuracy: 0.1281 - val_loss: 2.7296 - val_accuracy: 0.0824\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.6415 - accuracy: 0.1449\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.7296 to: 2.5585\n",
      "54/54 [==============================] - 29s 538ms/step - loss: 2.6430 - accuracy: 0.1434 - val_loss: 2.5585 - val_accuracy: 0.0853\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.5635 - accuracy: 0.1497\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.5585 to: 2.4469\n",
      "54/54 [==============================] - 29s 534ms/step - loss: 2.5633 - accuracy: 0.1492 - val_loss: 2.4469 - val_accuracy: 0.1588\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.5353 - accuracy: 0.1437\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.4469 to: 2.4010\n",
      "54/54 [==============================] - 29s 534ms/step - loss: 2.5326 - accuracy: 0.1410 - val_loss: 2.4010 - val_accuracy: 0.1647\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.4780 - accuracy: 0.1617\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.4010 to: 2.3487\n",
      "54/54 [==============================] - 29s 533ms/step - loss: 2.4850 - accuracy: 0.1610 - val_loss: 2.3487 - val_accuracy: 0.1324\n",
      "Epoch 8/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.4603 - accuracy: 0.1808\n",
      "Case_2 Val_loss did not improve. Best is: 2.3487. Not Caching\n",
      "54/54 [==============================] - 29s 538ms/step - loss: 2.4567 - accuracy: 0.1810 - val_loss: 2.3582 - val_accuracy: 0.1324\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.4360 - accuracy: 0.1725\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.3487 to: 2.2697\n",
      "54/54 [==============================] - 29s 539ms/step - loss: 2.4363 - accuracy: 0.1716 - val_loss: 2.2697 - val_accuracy: 0.1824\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.4036 - accuracy: 0.1784\n",
      "Case_2 Val_loss did not improve. Best is: 2.2697. Not Caching\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.4026 - accuracy: 0.1774 - val_loss: 2.3009 - val_accuracy: 0.1353\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.3617 - accuracy: 0.1964\n",
      "Case_2 Val_loss did not improve. Best is: 2.2697. Not Caching\n",
      "54/54 [==============================] - 29s 533ms/step - loss: 2.3614 - accuracy: 0.1974 - val_loss: 2.2760 - val_accuracy: 0.1794\n",
      "Epoch 12/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.3148 - accuracy: 0.2012\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.2697 to: 2.1566\n",
      "54/54 [==============================] - 29s 537ms/step - loss: 2.3227 - accuracy: 0.2009 - val_loss: 2.1566 - val_accuracy: 0.2176\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.3182 - accuracy: 0.1916\n",
      "Case_2 Val_loss did not improve. Best is: 2.1566. Not Caching\n",
      "54/54 [==============================] - 29s 535ms/step - loss: 2.3196 - accuracy: 0.1939 - val_loss: 2.1729 - val_accuracy: 0.1441\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.2612 - accuracy: 0.2193\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.1566 to: 2.0394\n",
      "54/54 [==============================] - 29s 543ms/step - loss: 2.2613 - accuracy: 0.2211 - val_loss: 2.0394 - val_accuracy: 0.2588\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.2248 - accuracy: 0.2180\n",
      "Case_2 Val_loss did not improve. Best is: 2.0394. Not Caching\n",
      "54/54 [==============================] - 29s 533ms/step - loss: 2.2238 - accuracy: 0.2174 - val_loss: 2.0867 - val_accuracy: 0.2176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1941 - accuracy: 0.2108\n",
      "Case_2 Val_loss did not improve. Best is: 2.0394. Not Caching\n",
      "54/54 [==============================] - 29s 537ms/step - loss: 2.1942 - accuracy: 0.2127 - val_loss: 2.0415 - val_accuracy: 0.2235\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1905 - accuracy: 0.2467\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 2.0394 to: 1.9842\n",
      "54/54 [==============================] - 29s 531ms/step - loss: 2.1945 - accuracy: 0.2456 - val_loss: 1.9842 - val_accuracy: 0.2647\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1363 - accuracy: 0.2683\n",
      "Case_2 Val_loss did not improve. Best is: 1.9842. Not Caching\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.1366 - accuracy: 0.2656 - val_loss: 2.1082 - val_accuracy: 0.2324\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1119 - accuracy: 0.2599\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.9842 to: 1.9241\n",
      "54/54 [==============================] - 29s 536ms/step - loss: 2.1168 - accuracy: 0.2573 - val_loss: 1.9241 - val_accuracy: 0.3353\n",
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0774 - accuracy: 0.2754\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.9241 to: 1.8839\n",
      "54/54 [==============================] - 29s 535ms/step - loss: 2.0789 - accuracy: 0.2738 - val_loss: 1.8839 - val_accuracy: 0.3441\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0302 - accuracy: 0.2983\n",
      "Case_2 Val_loss did not improve. Best is: 1.8839. Not Caching\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.0373 - accuracy: 0.2973 - val_loss: 2.2598 - val_accuracy: 0.2500\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0578 - accuracy: 0.2754\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.8839 to: 1.8285\n",
      "54/54 [==============================] - 29s 536ms/step - loss: 2.0644 - accuracy: 0.2714 - val_loss: 1.8285 - val_accuracy: 0.3971\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9711 - accuracy: 0.3222\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.8285 to: 1.6735\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 1.9690 - accuracy: 0.3231 - val_loss: 1.6735 - val_accuracy: 0.4529\n",
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9712 - accuracy: 0.3174\n",
      "Case_2 Val_loss did not improve. Best is: 1.6735. Not Caching\n",
      "54/54 [==============================] - 27s 506ms/step - loss: 1.9676 - accuracy: 0.3149 - val_loss: 1.7600 - val_accuracy: 0.3882\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9177 - accuracy: 0.3461\n",
      "Case_2 Val_loss did not improve. Best is: 1.6735. Not Caching\n",
      "54/54 [==============================] - 27s 503ms/step - loss: 1.9151 - accuracy: 0.3467 - val_loss: 1.8208 - val_accuracy: 0.3765\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9288 - accuracy: 0.3186\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.6735 to: 1.5877\n",
      "54/54 [==============================] - 27s 499ms/step - loss: 1.9273 - accuracy: 0.3220 - val_loss: 1.5877 - val_accuracy: 0.4912\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9006 - accuracy: 0.3437\n",
      "Case_2 Val_loss did not improve. Best is: 1.5877. Not Caching\n",
      "54/54 [==============================] - 27s 499ms/step - loss: 1.8992 - accuracy: 0.3431 - val_loss: 1.5893 - val_accuracy: 0.4912\n",
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8717 - accuracy: 0.3365\n",
      "Case_2 Val_loss did not improve. Best is: 1.5877. Not Caching\n",
      "54/54 [==============================] - 27s 500ms/step - loss: 1.8659 - accuracy: 0.3384 - val_loss: 1.6736 - val_accuracy: 0.4235\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8628 - accuracy: 0.3605\n",
      "Case_2 - Caching Checkpoint - val_loss has improved from: 1.5877 to: 1.5061\n",
      "54/54 [==============================] - 27s 501ms/step - loss: 1.8600 - accuracy: 0.3631 - val_loss: 1.5061 - val_accuracy: 0.5176\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7784 - accuracy: 0.3677\n",
      "Case_2 Val_loss did not improve. Best is: 1.5061. Not Caching\n",
      "54/54 [==============================] - 27s 501ms/step - loss: 1.7796 - accuracy: 0.3678 - val_loss: 1.6390 - val_accuracy: 0.4588\n",
      "Case_2 - Loading previous training data\n",
      "Case_2 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BOOST.hdf5\n",
      "Case_2 - Loaded best val_loss value: 1.5061 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_2BOOST_val_loss.txt\n",
      "Alex_Net has missed 496 data instances\n",
      "VGG_16 - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8309 - accuracy: 0.0825\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: inf to: 2.8349\n",
      "54/54 [==============================] - 228s 4s/step - loss: 2.8308 - accuracy: 0.0810 - val_loss: 2.8349 - val_accuracy: 0.0471\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8257 - accuracy: 0.0826\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8258 - accuracy: 0.0823 - val_loss: 2.8370 - val_accuracy: 0.0441\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8213 - accuracy: 0.0838\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8208 - accuracy: 0.0870 - val_loss: 2.8401 - val_accuracy: 0.0441\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8164 - accuracy: 0.0898\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8157 - accuracy: 0.0893 - val_loss: 2.8439 - val_accuracy: 0.0441\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8101 - accuracy: 0.0946\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8100 - accuracy: 0.0928 - val_loss: 2.8522 - val_accuracy: 0.0441\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8020 - accuracy: 0.0922\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8029 - accuracy: 0.0917 - val_loss: 2.8648 - val_accuracy: 0.0824\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.7972 - accuracy: 0.0970\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 222s 4s/step - loss: 2.8000 - accuracy: 0.0952 - val_loss: 2.8732 - val_accuracy: 0.0735\n",
      "Epoch 8/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.7973 - accuracy: 0.0958\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.7982 - accuracy: 0.0964 - val_loss: 2.8783 - val_accuracy: 0.0647\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.7879 - accuracy: 0.0934\n",
      "VGG_16 Val_loss did not improve. Best is: 2.8349. Not Caching\n",
      "54/54 [==============================] - 222s 4s/step - loss: 2.7893 - accuracy: 0.0940 - val_loss: 2.8497 - val_accuracy: 0.0676\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.7453 - accuracy: 0.0994\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.8349 to: 2.7457\n",
      "54/54 [==============================] - 222s 4s/step - loss: 2.7456 - accuracy: 0.0999 - val_loss: 2.7457 - val_accuracy: 0.0765\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.6166 - accuracy: 0.1377\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.7457 to: 2.5376\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.6173 - accuracy: 0.1387 - val_loss: 2.5376 - val_accuracy: 0.1324\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/54 [============================>.] - ETA: 3s - loss: 2.6429 - accuracy: 0.1425\n",
      "VGG_16 Val_loss did not improve. Best is: 2.5376. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.6469 - accuracy: 0.1410 - val_loss: 2.8545 - val_accuracy: 0.0471\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8147 - accuracy: 0.0766\n",
      "VGG_16 Val_loss did not improve. Best is: 2.5376. Not Caching\n",
      "54/54 [==============================] - 221s 4s/step - loss: 2.8143 - accuracy: 0.0764 - val_loss: 2.8569 - val_accuracy: 0.0471\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.8015 - accuracy: 0.0814\n",
      "VGG_16 Val_loss did not improve. Best is: 2.5376. Not Caching\n",
      "54/54 [==============================] - 229s 4s/step - loss: 2.8032 - accuracy: 0.0811 - val_loss: 2.8602 - val_accuracy: 0.0441\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.7898 - accuracy: 0.1018\n",
      "VGG_16 Val_loss did not improve. Best is: 2.5376. Not Caching\n",
      "54/54 [==============================] - 243s 4s/step - loss: 2.7927 - accuracy: 0.0999 - val_loss: 2.8565 - val_accuracy: 0.0706\n",
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.6519 - accuracy: 0.1341\n",
      "VGG_16 Val_loss did not improve. Best is: 2.5376. Not Caching\n",
      "54/54 [==============================] - 240s 4s/step - loss: 2.6542 - accuracy: 0.1351 - val_loss: 2.6815 - val_accuracy: 0.0676\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.4252 - accuracy: 0.1832\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.5376 to: 2.3636\n",
      "54/54 [==============================] - 232s 4s/step - loss: 2.4216 - accuracy: 0.1845 - val_loss: 2.3636 - val_accuracy: 0.1500\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.3706 - accuracy: 0.1988\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.3636 to: 2.2982\n",
      "54/54 [==============================] - 233s 4s/step - loss: 2.3630 - accuracy: 0.1998 - val_loss: 2.2982 - val_accuracy: 0.2088\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.3153 - accuracy: 0.1952\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.2982 to: 2.2385\n",
      "54/54 [==============================] - 243s 5s/step - loss: 2.3138 - accuracy: 0.1951 - val_loss: 2.2385 - val_accuracy: 0.2118\n",
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.2546 - accuracy: 0.2072\n",
      "VGG_16 Val_loss did not improve. Best is: 2.2385. Not Caching\n",
      "54/54 [==============================] - 245s 5s/step - loss: 2.2502 - accuracy: 0.2092 - val_loss: 2.3333 - val_accuracy: 0.2382\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.2320 - accuracy: 0.2287\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.2385 to: 2.1544\n",
      "54/54 [==============================] - 245s 5s/step - loss: 2.2366 - accuracy: 0.2268 - val_loss: 2.1544 - val_accuracy: 0.2147\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.2176 - accuracy: 0.2144\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.1544 to: 2.1249\n",
      "54/54 [==============================] - 243s 5s/step - loss: 2.2165 - accuracy: 0.2186 - val_loss: 2.1249 - val_accuracy: 0.2235\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.1857 - accuracy: 0.2335\n",
      "VGG_16 Val_loss did not improve. Best is: 2.1249. Not Caching\n",
      "54/54 [==============================] - 240s 4s/step - loss: 2.1847 - accuracy: 0.2362 - val_loss: 2.1317 - val_accuracy: 0.2676\n",
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.1445 - accuracy: 0.2491\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.1249 to: 2.1218\n",
      "54/54 [==============================] - 242s 4s/step - loss: 2.1509 - accuracy: 0.2456 - val_loss: 2.1218 - val_accuracy: 0.2529\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 3s - loss: 2.1450 - accuracy: 0.2299\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.1218 to: 2.0552\n",
      "54/54 [==============================] - 242s 4s/step - loss: 2.1490 - accuracy: 0.2291 - val_loss: 2.0552 - val_accuracy: 0.2647\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.1160 - accuracy: 0.2647\n",
      "VGG_16 Val_loss did not improve. Best is: 2.0552. Not Caching\n",
      "54/54 [==============================] - 252s 5s/step - loss: 2.1096 - accuracy: 0.2691 - val_loss: 2.1412 - val_accuracy: 0.2588\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.0764 - accuracy: 0.2618\n",
      "VGG_16 Val_loss did not improve. Best is: 2.0552. Not Caching\n",
      "54/54 [==============================] - 260s 5s/step - loss: 2.0761 - accuracy: 0.2627 - val_loss: 2.0773 - val_accuracy: 0.2588\n",
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.0646 - accuracy: 0.2491\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.0552 to: 2.0281\n",
      "54/54 [==============================] - 273s 5s/step - loss: 2.0664 - accuracy: 0.2468 - val_loss: 2.0281 - val_accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.0481 - accuracy: 0.2743\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.0281 to: 2.0071\n",
      "54/54 [==============================] - 276s 5s/step - loss: 2.0387 - accuracy: 0.2773 - val_loss: 2.0071 - val_accuracy: 0.3088\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 4s - loss: 2.0257 - accuracy: 0.2766\n",
      "VGG_16 - Caching Checkpoint - val_loss has improved from: 2.0071 to: 1.8800\n",
      "54/54 [==============================] - 247s 5s/step - loss: 2.0236 - accuracy: 0.2785 - val_loss: 1.8800 - val_accuracy: 0.3206\n",
      "VGG_16 - Loading previous training data\n",
      "VGG_16 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BOOST.hdf5\n",
      "VGG_16 - Loaded best val_loss value: 1.8800 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//VGG_16BOOST_val_loss.txt\n",
      "Case_2 has missed 576 data instances\n",
      "Case_3 - Training for 30 epochs\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 54 steps, validate on 340 samples\n",
      "Epoch 1/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.8287 - accuracy: 0.0707\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: inf to: 2.8342\n",
      "54/54 [==============================] - 35s 640ms/step - loss: 2.8287 - accuracy: 0.0705 - val_loss: 2.8342 - val_accuracy: 0.0441\n",
      "Epoch 2/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.8132 - accuracy: 0.1014\n",
      "Case_3 Val_loss did not improve. Best is: 2.8342. Not Caching\n",
      "54/54 [==============================] - 31s 569ms/step - loss: 2.8134 - accuracy: 0.1007 - val_loss: 2.8405 - val_accuracy: 0.0441\n",
      "Epoch 3/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7982 - accuracy: 0.0970\n",
      "Case_3 Val_loss did not improve. Best is: 2.8342. Not Caching\n",
      "54/54 [==============================] - 30s 554ms/step - loss: 2.7986 - accuracy: 0.0999 - val_loss: 2.8484 - val_accuracy: 0.0441\n",
      "Epoch 4/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7776 - accuracy: 0.1138\n",
      "Case_3 Val_loss did not improve. Best is: 2.8342. Not Caching\n",
      "54/54 [==============================] - 31s 565ms/step - loss: 2.7773 - accuracy: 0.1140 - val_loss: 2.8547 - val_accuracy: 0.0500\n",
      "Epoch 5/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.7479 - accuracy: 0.1641\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.8342 to: 2.8247\n",
      "54/54 [==============================] - 31s 576ms/step - loss: 2.7449 - accuracy: 0.1657 - val_loss: 2.8247 - val_accuracy: 0.1000\n",
      "Epoch 6/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.6620 - accuracy: 0.1401\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.8247 to: 2.6750\n",
      "54/54 [==============================] - 35s 653ms/step - loss: 2.6567 - accuracy: 0.1422 - val_loss: 2.6750 - val_accuracy: 0.1441\n",
      "Epoch 7/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.5186 - accuracy: 0.1629\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.6750 to: 2.4823\n",
      "54/54 [==============================] - 34s 627ms/step - loss: 2.5155 - accuracy: 0.1645 - val_loss: 2.4823 - val_accuracy: 0.1294\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/54 [============================>.] - ETA: 0s - loss: 2.3690 - accuracy: 0.1851\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.4823 to: 2.2720\n",
      "54/54 [==============================] - 33s 612ms/step - loss: 2.3685 - accuracy: 0.1852 - val_loss: 2.2720 - val_accuracy: 0.1824\n",
      "Epoch 9/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.3624 - accuracy: 0.1952\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.2720 to: 2.2459\n",
      "54/54 [==============================] - 32s 601ms/step - loss: 2.3623 - accuracy: 0.1939 - val_loss: 2.2459 - val_accuracy: 0.1765\n",
      "Epoch 10/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.2603 - accuracy: 0.2084\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.2459 to: 2.1778\n",
      "54/54 [==============================] - 33s 612ms/step - loss: 2.2609 - accuracy: 0.2056 - val_loss: 2.1778 - val_accuracy: 0.1941\n",
      "Epoch 11/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1758 - accuracy: 0.2488\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.1778 to: 2.0961\n",
      "54/54 [==============================] - 31s 580ms/step - loss: 2.1781 - accuracy: 0.2488 - val_loss: 2.0961 - val_accuracy: 0.2676\n",
      "Epoch 12/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.1293 - accuracy: 0.2719\n",
      "Case_3 Val_loss did not improve. Best is: 2.0961. Not Caching\n",
      "54/54 [==============================] - 30s 559ms/step - loss: 2.1296 - accuracy: 0.2691 - val_loss: 2.1061 - val_accuracy: 0.2176\n",
      "Epoch 13/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0813 - accuracy: 0.2563\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.0961 to: 2.0411\n",
      "54/54 [==============================] - 37s 683ms/step - loss: 2.0783 - accuracy: 0.2573 - val_loss: 2.0411 - val_accuracy: 0.2382\n",
      "Epoch 14/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0429 - accuracy: 0.2790\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.0411 to: 2.0046\n",
      "54/54 [==============================] - 39s 715ms/step - loss: 2.0444 - accuracy: 0.2785 - val_loss: 2.0046 - val_accuracy: 0.2559\n",
      "Epoch 15/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 2.0316 - accuracy: 0.2754\n",
      "Case_3 Val_loss did not improve. Best is: 2.0046. Not Caching\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.0302 - accuracy: 0.2773 - val_loss: 2.0294 - val_accuracy: 0.2500\n",
      "Epoch 16/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9843 - accuracy: 0.3018\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 2.0046 to: 1.9239\n",
      "54/54 [==============================] - 40s 738ms/step - loss: 1.9906 - accuracy: 0.2996 - val_loss: 1.9239 - val_accuracy: 0.3118\n",
      "Epoch 17/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9557 - accuracy: 0.2886\n",
      "Case_3 Val_loss did not improve. Best is: 1.9239. Not Caching\n",
      "54/54 [==============================] - 40s 740ms/step - loss: 1.9529 - accuracy: 0.2902 - val_loss: 1.9933 - val_accuracy: 0.2971\n",
      "Epoch 18/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9149 - accuracy: 0.3293\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.9239 to: 1.7952\n",
      "54/54 [==============================] - 33s 612ms/step - loss: 1.9131 - accuracy: 0.3290 - val_loss: 1.7952 - val_accuracy: 0.3647\n",
      "Epoch 19/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.9033 - accuracy: 0.3246\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 37s 685ms/step - loss: 1.9015 - accuracy: 0.3243 - val_loss: 1.9640 - val_accuracy: 0.2824\n",
      "Epoch 20/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8759 - accuracy: 0.3365\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 31s 566ms/step - loss: 1.8649 - accuracy: 0.3443 - val_loss: 1.9119 - val_accuracy: 0.3265\n",
      "Epoch 21/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8418 - accuracy: 0.3485\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 33s 606ms/step - loss: 1.8342 - accuracy: 0.3514 - val_loss: 1.9355 - val_accuracy: 0.3265\n",
      "Epoch 22/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.8170 - accuracy: 0.3514\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 31s 573ms/step - loss: 1.8248 - accuracy: 0.3507 - val_loss: 1.8984 - val_accuracy: 0.3235\n",
      "Epoch 23/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6869 - accuracy: 0.4024\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 28s 527ms/step - loss: 1.6971 - accuracy: 0.4031 - val_loss: 1.8959 - val_accuracy: 0.3294\n",
      "Epoch 24/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7647 - accuracy: 0.3760\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 30s 552ms/step - loss: 1.7611 - accuracy: 0.3772 - val_loss: 2.1860 - val_accuracy: 0.2382\n",
      "Epoch 25/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.7330 - accuracy: 0.3772\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 32s 589ms/step - loss: 1.7349 - accuracy: 0.3760 - val_loss: 1.8630 - val_accuracy: 0.3471\n",
      "Epoch 26/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6713 - accuracy: 0.3976\n",
      "Case_3 Val_loss did not improve. Best is: 1.7952. Not Caching\n",
      "54/54 [==============================] - 35s 651ms/step - loss: 1.6702 - accuracy: 0.4007 - val_loss: 1.8233 - val_accuracy: 0.3441\n",
      "Epoch 27/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6210 - accuracy: 0.4228\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.7952 to: 1.7883\n",
      "54/54 [==============================] - 37s 684ms/step - loss: 1.6192 - accuracy: 0.4242 - val_loss: 1.7883 - val_accuracy: 0.3971\n",
      "Epoch 28/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6355 - accuracy: 0.4060\n",
      "Case_3 Val_loss did not improve. Best is: 1.7883. Not Caching\n",
      "54/54 [==============================] - 36s 663ms/step - loss: 1.6315 - accuracy: 0.4089 - val_loss: 1.8816 - val_accuracy: 0.3412\n",
      "Epoch 29/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5696 - accuracy: 0.4311\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.7883 to: 1.7796\n",
      "54/54 [==============================] - 37s 678ms/step - loss: 1.5668 - accuracy: 0.4336 - val_loss: 1.7796 - val_accuracy: 0.3706\n",
      "Epoch 30/30\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.5779 - accuracy: 0.4132\n",
      "Case_3 - Caching Checkpoint - val_loss has improved from: 1.7796 to: 1.6321\n",
      "54/54 [==============================] - 37s 689ms/step - loss: 1.5778 - accuracy: 0.4125 - val_loss: 1.6321 - val_accuracy: 0.4059\n",
      "Case_3 - Loading previous training data\n",
      "Case_3 - Loaded previous model weights from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BOOST.hdf5\n",
      "Case_3 - Loaded best val_loss value: 1.6321 from: C:\\Users\\danie\\Documents\\Work\\AI_portfolio\\AI_portfolio/cached/deeplearning//Case_3BOOST_val_loss.txt\n",
      "Training CNN architectures complete\n",
      "Gathering final results\n",
      "Predicting completed. Final Ensemble Loss: 0.1010 Final Ensemble Accuracy: 67.94%\n",
      "Alex_net Loss: 1.4700 Accuracy: 45.59%\n",
      "VGG_16_net Loss: 1.8800 Accuracy: 32.06%\n",
      "Case_0_net Loss: 1.1273 Accuracy: 60.59%\n",
      "Case_1_net Loss: 1.0643 Accuracy: 66.76%\n",
      "Case_2_net Loss: 1.5061 Accuracy: 51.76%\n",
      "Case_3_net Loss: 1.6321 Accuracy: 40.59%\n"
     ]
    }
   ],
   "source": [
    "# Create Ensemble method\n",
    "# This setup allows incremental training\n",
    "SHOULD_TRAIN = True       # true if models should be trained\n",
    "TRAIN_FOR_NUM_EPOCHS = 30 # the number of epochs to run if training\n",
    "LOAD_WEIGHTS = True       # set to false to restart training\n",
    "\n",
    "SPLIT_RATIO = 0.85          # the bag size used relative to the whole dataset\n",
    "\n",
    "# 1. Create the used architectures. This will also load cached best model if exists\n",
    "print(\"Initialising CNN architectures\")\n",
    "alex_net   = alex_net_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "vgg_16_net = vgg_16_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "case_0_net = case_0_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "case_1_net = case_1_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "case_2_net = case_2_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "case_3_net = case_3_CNN_augmented_checkpointed(LOAD_WEIGHTS, \"BOOST\")\n",
    "\n",
    "\n",
    "if SHOULD_TRAIN:\n",
    "  data_size = len(trainX)\n",
    "  size = int(data_size * SPLIT_RATIO)\n",
    "\n",
    "  # each model will be trained here. Once training is complete, the checkpointed weights\n",
    "  # are loaded back in. This is to allow the final tests to be done on the best performing model\n",
    "  print(\"Training CNN architectures for {} epochs\".format(TRAIN_FOR_NUM_EPOCHS))\n",
    "  indices = np.random.choice(data_size, size, replace=False)\n",
    "  # Get the randomly selected data\n",
    "  split_data_x = trainX[indices]\n",
    "  split_data_y = trainY[indices]\n",
    "  print(\"Alex_Net will train on {} data instances\".format(len(split_data_x)))\n",
    "  # Train\n",
    "  alex_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  alex_net.load_checkpointed_weights()\n",
    "  # Get indices of miss classified instances\n",
    "  missed_indices = alex_net.get_instances_incorrectly_predicted(split_data_x, split_data_y)\n",
    "\n",
    "  # Keep the miss classified data in the array\n",
    "  split_data_x = split_data_x[missed_indices]\n",
    "  split_data_y = split_data_y[missed_indices]\n",
    "  num_missed = len(split_data_x)\n",
    "  print(\"AlexNet has missed {} data instances\".format(num_missed))\n",
    "  # Randomly choose the rest to fill up the array\n",
    "  indices = np.random.choice(data_size, (size - num_missed), replace=False)\n",
    "  split_data_x = np.append(split_data_x, trainX[indices], axis=0)\n",
    "  split_data_y = np.append(split_data_y, trainY[indices], axis=0)\n",
    "  # Train\n",
    "  case_0_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  case_0_net.load_checkpointed_weights()\n",
    "  # Get indices of miss classified instances\n",
    "  missed_indices = case_0_net.get_instances_incorrectly_predicted(split_data_x, split_data_y)\n",
    "\n",
    "    \n",
    "  # Keep the miss classified data in the array\n",
    "  split_data_x = split_data_x[missed_indices]\n",
    "  split_data_y = split_data_y[missed_indices]\n",
    "  # Randomly choose the rest to fill up the array\n",
    "  num_missed = len(split_data_x)\n",
    "  print(\"Case_0 has missed {} data instances\".format(num_missed))\n",
    "  indices = np.random.choice(data_size, (size - num_missed), replace=False)\n",
    "  split_data_x = np.append(split_data_x, trainX[indices], axis=0)\n",
    "  split_data_y = np.append(split_data_y, trainY[indices], axis=0)\n",
    "  # Train\n",
    "  case_1_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  case_1_net.load_checkpointed_weights()\n",
    "  # Get indices of miss classified instances\n",
    "  missed_indices = case_1_net.get_instances_incorrectly_predicted(split_data_x, split_data_y)\n",
    "\n",
    "  # Keep the miss classified data in the array\n",
    "  split_data_x = split_data_x[missed_indices]\n",
    "  split_data_y = split_data_y[missed_indices]\n",
    "  # Randomly choose the rest to fill up the array\n",
    "  num_missed = len(split_data_x)\n",
    "  print(\"Case_1 has missed {} data instances\".format(num_missed))\n",
    "  indices = np.random.choice(data_size, (size - num_missed), replace=False)\n",
    "  split_data_x = np.append(split_data_x, trainX[indices], axis=0)\n",
    "  split_data_y = np.append(split_data_y, trainY[indices], axis=0)\n",
    "  # Train\n",
    "  case_2_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  case_2_net.load_checkpointed_weights()\n",
    "  # Get indices of miss classified instances\n",
    "  missed_indices = case_2_net.get_instances_incorrectly_predicted(split_data_x, split_data_y)\n",
    "\n",
    "    \n",
    "  # Keep the miss classified data in the array\n",
    "  split_data_x = split_data_x[missed_indices]\n",
    "  split_data_y = split_data_y[missed_indices]\n",
    "  # Randomly choose the rest to fill up the array\n",
    "  num_missed = len(split_data_x)\n",
    "  print(\"Case 2 has missed {} data instances\".format(num_missed))\n",
    "  indices = np.random.choice(data_size, (size - num_missed), replace=False)\n",
    "  split_data_x = np.append(split_data_x, trainX[indices], axis=0)\n",
    "  split_data_y = np.append(split_data_y, trainY[indices], axis=0)\n",
    "  # Train\n",
    "  vgg_16_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  vgg_16_net.load_checkpointed_weights()\n",
    "  # Get indices of miss classified instances\n",
    "  missed_indices = vgg_16_net.get_instances_incorrectly_predicted(split_data_x, split_data_y)\n",
    "\n",
    "    \n",
    "  # Keep the miss classified data in the array\n",
    "  split_data_x = split_data_x[missed_indices]\n",
    "  split_data_y = split_data_y[missed_indices]\n",
    "  # Randomly choose the rest to fill up the array\n",
    "  num_missed = len(split_data_x)\n",
    "  print(\"VGG_16 has missed {} data instances\".format(num_missed))\n",
    "  indices = np.random.choice(data_size, (size - num_missed), replace=False)\n",
    "  split_data_x = np.append(split_data_x, trainX[indices], axis=0)\n",
    "  split_data_y = np.append(split_data_y, trainY[indices], axis=0)\n",
    "  # Train\n",
    "  case_3_net.train(split_data_x, split_data_y, testX, testY, TRAIN_FOR_NUM_EPOCHS)\n",
    "  # Reload best weights\n",
    "  case_3_net.load_checkpointed_weights()\n",
    "\n",
    "  print(\"Training CNN architectures complete\")\n",
    "\n",
    "print(\"Gathering final results\")\n",
    "\n",
    "prediction_a = alex_net.predict(testX)\n",
    "prediction_v = vgg_16_net.predict(testX)\n",
    "prediction_0 = case_0_net.predict(testX)\n",
    "prediction_1 = case_1_net.predict(testX)\n",
    "prediction_2 = case_2_net.predict(testX)\n",
    "prediction_3 = case_3_net.predict(testX)\n",
    "\n",
    "total_predictions = prediction_a + prediction_v + prediction_0 + prediction_1 + prediction_2 + prediction_3\n",
    "\n",
    "final_loss, final_accuracy = get_prediction_result(total_predictions, testY)\n",
    "\n",
    "print(\"Predicting completed. Final Ensemble Loss: {:.4f} Final Ensemble Accuracy: {:.2f}%\".format(final_loss, final_accuracy*100.0))\n",
    "\n",
    "# Print Individual algorithm accuracy\n",
    "alex_net_loss, alex_net_accuracy          = get_prediction_result(prediction_a, testY)\n",
    "vgg_16_net_loss, vgg_16_net_accuracy      = get_prediction_result(prediction_v, testY)\n",
    "case_0_net_loss, case_0_net_accuracy      = get_prediction_result(prediction_0, testY)\n",
    "case_1_net_loss, case_1_net_accuracy      = get_prediction_result(prediction_1, testY)\n",
    "case_2_net_loss, case_2_net_accuracy      = get_prediction_result(prediction_2, testY)\n",
    "case_3_net_loss, case_3_net_accuracy      = get_prediction_result(prediction_3, testY)\n",
    "\n",
    "print(\"Alex_net Loss: {:.4f} Accuracy: {:.2f}%\".format(alex_net_loss, alex_net_accuracy*100.0))\n",
    "print(\"VGG_16_net Loss: {:.4f} Accuracy: {:.2f}%\".format(vgg_16_net_loss, vgg_16_net_accuracy*100.0))\n",
    "print(\"Case_0_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_0_net_loss, case_0_net_accuracy*100.0))\n",
    "print(\"Case_1_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_1_net_loss, case_1_net_accuracy*100.0))\n",
    "print(\"Case_2_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_2_net_loss, case_2_net_accuracy*100.0))\n",
    "print(\"Case_3_net Loss: {:.4f} Accuracy: {:.2f}%\".format(case_3_net_loss, case_3_net_accuracy*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PartA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
